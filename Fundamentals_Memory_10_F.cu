When you allocate a 2‑D array with the regular `cudaMalloc` and copy it with `cudaMemcpy`, you are treating the data as a single linear block. The code will run correctly because the host buffer is laid out contiguously and the copy operation simply copies that whole block to device memory. However, this approach misses several performance advantages that the *pitch* (or “2‑D”) API gives you:

| Issue | Why it hurts performance |
|-------|---------------------------|
| **No automatic row padding** | `cudaMallocPitch` guarantees that each row starts on a 32‑byte boundary and that the pitch is a multiple of 128 bytes (on most GPUs). If you just use `cudaMalloc`, you have to manage any padding yourself. Without proper padding the device memory accesses in a kernel can become non‑coalesced, forcing the memory controller to issue more transactions. |
| **Sub‑optimal coalescing for 2‑D accesses** | When a thread block reads a 2‑D tile, the kernel usually wants to read a contiguous 2‑D slice. If the data is not laid out with the right pitch, the reads cross 32‑byte boundaries, breaking the hardware’s ability to merge them into a single memory transaction. |
| **Extra bandwidth during copy** | `cudaMemcpy` copies a contiguous region. If your logical 2‑D array would normally have padding between rows (which `cudaMallocPitch` supplies), `cudaMemcpy` will copy only the data you actually requested, but you’ll lose the benefits of the hardware’s 2‑D copy path (`cudaMemcpy2D`). That path can copy each row using a single transaction per row, which is usually faster than copying one huge block in one go. |
| **Cache and TLB thrashing** | A linear allocation can cause the L1/L2 caches and TLB to be less effective when kernels access the array in a 2‑D pattern. Pitch allocation tends to align each row to cache line boundaries, reducing miss rates. |
| **No built‑in safety for over‑reads** | With pitch you can intentionally allocate a slightly larger buffer and still guarantee that any out‑of‑bounds accesses stay within the allocated space. With a flat allocation you risk over‑reading/writing past the end of the buffer. |

In short, using `cudaMalloc` + `cudaMemcpy` for a 2‑D array works because the device can still store the data, but you lose the alignment, coalescing, and copy‑optimization benefits that `cudaMallocPitch` / `cudaMemcpy2D` provide. The kernel will therefore likely make more memory transactions and the copy operation will use more bandwidth, leading to slower overall performance.