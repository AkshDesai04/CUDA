{
"Fundamentals_Setup_01": "Write a CUDA program to query and print the number of CUDA-enabled devices available on your system. The program should use the `cudaGetDeviceCount` API call and print a user-friendly message, such as 'Found X CUDA-capable devices.'",
"Fundamentals_Setup_01_A": "Write a program that simply checks if there is at least one CUDA-capable device and prints 'CUDA is available!' or 'No CUDA devices found.'",
"Fundamentals_Setup_01_B": "Write a program to get the ID of the currently active CUDA device using `cudaGetDevice` and print it.",
"Fundamentals_Setup_01_C": "Write a program to explicitly set the active CUDA device to 0 using `cudaSetDevice`. Verify the call was successful by then calling `cudaGetDevice` and printing the result.",
"Fundamentals_Setup_01_D": "Query and print the number of multiprocessors on the primary CUDA device (device 0).",
"Fundamentals_Setup_01_E": "Query and print the memory clock rate of the primary CUDA device in kilohertz (kHz).",
"Fundamentals_Setup_01_F": "Query and print the amount of shared memory available per thread block in bytes for the primary device.",
"Fundamentals_Setup_01_G": "Query and print the total number of registers available per thread block on the primary device.",
"Fundamentals_Setup_01_H": "Write a program to determine if the primary GPU is an integrated (sharing memory with the CPU) or a discrete device. Print the result.",
"Fundamentals_Setup_01_I": "Query and print the PCI Bus ID and PCI Device ID for the primary GPU.",
"Fundamentals_Setup_01_J": "Query if the primary device has ECC (Error-Correcting Code) memory enabled and print the result.",
"Fundamentals_Setup_01_K": "Write a program that attempts to set the device to an invalid index (e.g., 999) and use proper error checking to report that the device does not exist.",
"Fundamentals_Setup_01_L": "Query and print the maximum number of threads that can be active on a single multiprocessor for the primary device.",
"Fundamentals_Setup_01_M": "Query and print the major and minor compute capability numbers for the primary CUDA device separately.",
"Fundamentals_Setup_02": "Extend the previous program to iterate through all available CUDA devices. For each device, use `cudaGetDeviceProperties` to retrieve its properties and print its name (e.g., 'NVIDIA GeForce RTX 3080') and its compute capability (e.g., 'Compute Capability: 8.6').",
"Fundamentals_Setup_02_A": "Iterate through all available CUDA devices and print their total global memory in gigabytes (GiB) for each.",
"Fundamentals_Setup_02_B": "Iterate through all devices and, for each one, print its multiprocessor count.",
"Fundamentals_Setup_02_C": "Iterate through all devices and print the warp size for each one.",
"Fundamentals_Setup_02_D": "Iterate through all devices and print the maximum number of threads per block for each.",
"Fundamentals_Setup_02_E": "Write a program that finds the device with the most global memory and prints its name and index.",
"Fundamentals_Setup_02_F": "Write a program that finds the device with the highest compute capability and prints its name and index.",
"Fundamentals_Setup_02_G": "For each device, print its memory bus width in bits.",
"Fundamentals_Setup_02_H": "For each device, print its L2 cache size in megabytes (MB).",
"Fundamentals_Setup_02_I": "For each device, query and print whether it supports running kernels concurrently.",
"Fundamentals_Setup_02_J": "For each device, print its maximum grid dimensions ([X, Y, Z]).",
"Fundamentals_Setup_02_K": "Write a program that creates a summary table, printing the ID, Name, and Global Memory for every CUDA device found.",
"Fundamentals_Setup_03": "Write a program that queries and prints the total amount of global memory available on device 0 in megabytes (MB). You will need to get the `totalGlobalMem` property from `cudaDeviceProp` and convert it from bytes to MB.",
"Fundamentals_Setup_03_A": "Query and print the total amount of constant memory available on device 0 in kilobytes (KB).",
"Fundamentals_Setup_03_B": "Query and print the total amount of shared memory available per multiprocessor on device 0 in kilobytes (KB).",
"Fundamentals_Setup_03_C": "Write a program that queries the available global memory and prints a warning if it is less than a certain threshold (e.g., 4096 MB).",
"Fundamentals_Setup_03_D": "Query and print the memory bus width of the primary GPU in bits.",
"Fundamentals_Setup_03_E": "Query and print the maximum memory pitch in bytes allowed for memory allocations.",
"Fundamentals_Setup_03_F": "Query and print the texture alignment requirement for device 0.",
"Fundamentals_Setup_03_G": "Query and print the maximum dimension of a 1D texture for device 0.",
"Fundamentals_Setup_03_H": "Query and print the maximum dimensions (width and height) of a 2D texture for device 0.",
"Fundamentals_Setup_03_I": "Calculate and print the theoretical peak memory bandwidth of the primary device in GB/s. Formula: `(memoryClockRate_kHz * 1000 * memoryBusWidth_bits / 8) / 1e9 * 2` (the *2 is for DDR memory).",
"Fundamentals_Setup_03_J": "Write a function `get_gpu_memory_mb(int device_id)` that returns the total global memory in MB for the given device.",
"Fundamentals_Setup_04": "Write a program to determine and print the 'warp size' for the default CUDA device. The warp size is a fundamental property that dictates how threads are scheduled. Find this value in the device properties struct.",
"Fundamentals_Setup_04_A": "After printing the warp size, add a comment explaining what a warp is and why its size is important for performance (e.g., memory coalescing, SIMT execution).",
"Fundamentals_Setup_04_B": "Query and print the multiprocessor count for the default device.",
"Fundamentals_Setup_04_C": "Query and print the maximum number of threads per multiprocessor for the default device.",
"Fundamentals_Setup_04_D": "Calculate and print the theoretical maximum number of concurrent threads the entire GPU can run (`maxThreadsPerMultiProcessor` * `multiProcessorCount`).",
"Fundamentals_Setup_04_E": "Calculate and print the theoretical maximum number of concurrent warps the entire GPU can run.",
"Fundamentals_Setup_04_F": "Query and print the `computeMode` of the primary GPU (e.g., Default, Exclusive, Prohibited).",
"Fundamentals_Setup_04_G": "Query and print the maximum number of blocks that can reside on a single multiprocessor.",
"Fundamentals_Setup_04_H": "Query and print the GPU's core clock rate in GHz.",
"Fundamentals_Setup_04_I": "Check if the GPU supports `unifiedAddressing` and print a confirmation message.",
"Fundamentals_Setup_04_J": "Query and print whether the device can map host memory (`canMapHostMemory` property).",
"Fundamentals_Setup_05": "Query and print the maximum number of threads allowed in a single thread block for the primary CUDA device. This value, `maxThreadsPerBlock`, is a key constraint when designing kernels.",
"Fundamentals_Setup_05_A": "Based on the `maxThreadsPerBlock` value and a warp size of 32, calculate and print the maximum number of warps per block.",
"Fundamentals_Setup_05_B": "Query and print the maximum number of registers available per block.",
"Fundamentals_Setup_05_C": "If a kernel uses 64 registers per thread and the max threads per block is 1024, calculate the total registers required by a full block. Compare this to the max registers per block to see if such a launch is possible.",
"Fundamentals_Setup_05_D": "Query and print the maximum amount of shared memory per block in bytes.",
"Fundamentals_Setup_05_E": "If a kernel requires 16KB of shared memory, calculate the maximum number of threads you can launch in a block if the device limit is 48KB per block and 1024 threads (assuming no other constraints).",
"Fundamentals_Setup_05_F": "Write a program that prints all key resource limits for a block: max threads, max shared memory, and max registers.",
"Fundamentals_Setup_05_G": "Query and print the maximum x-dimension of a thread block.",
"Fundamentals_Setup_05_H": "Query and print the maximum y-dimension of a thread block.",
"Fundamentals_Setup_05_I": "Query and print the maximum z-dimension of a thread block.",
"Fundamentals_Setup_05_J": "Explain in a comment why there is a limit on the number of threads per block.",
"Fundamentals_Setup_06": "Query and print the maximum dimensions (x, y, z) of a thread block for the primary CUDA device. The result should be displayed clearly, for example: 'Max Block Dimensions: [X, Y, Z]'.",
"Fundamentals_Setup_06_A": "From the maximum dimensions [X, Y, Z], verify that X * Y * Z is not necessarily equal to `maxThreadsPerBlock` (since X, Y, and Z are individual dimension limits).",
"Fundamentals_Setup_06_B": "Write a program that checks if a hypothetical block configuration of (1024, 2, 1) would be valid, given the device's limits.",
"Fundamentals_Setup_06_C": "Write a program that checks if a hypothetical block configuration of (32, 32, 2) would be valid.",
"Fundamentals_Setup_06_D": "Print the maximum x-dimension of a grid of blocks.",
"Fundamentals_Setup_06_E": "Print the maximum y-dimension of a grid of blocks.",
"Fundamentals_Setup_06_F": "Print the maximum z-dimension of a grid of blocks.",
"Fundamentals_Setup_06_G": "Explain in a comment the difference between block dimensions and grid dimensions.",
"Fundamentals_Setup_06_H": "Query and print the surface alignment requirement in bytes.",
"Fundamentals_Setup_06_I": "Query and print the maximum layered 1D texture size and number of layers.",
"Fundamentals_Setup_06_J": "Query and print the maximum layered 2D texture size and number of layers.",
"Fundamentals_Setup_07": "Query and print the maximum dimensions (x, y, z) of a grid of thread blocks for the primary CUDA device. This shows the maximum number of blocks you can launch in each dimension.",
"Fundamentals_Setup_07_A": "Explain in a comment what a grid of blocks is in the CUDA execution model.",
"Fundamentals_Setup_07_B": "Given the maximum grid dimensions, calculate the theoretical maximum number of blocks you could launch in total (though this is often limited by other factors).",
"Fundamentals_Setup_07_C": "Write a program that prints a summary of the execution hierarchy limits: Max Grid [X, Y, Z] and Max Block [X, Y, Z].",
"Fundamentals_Setup_07_D": "Query and check if the device supports Page-locked Memory Mapped On The GPU (`pageableMemoryAccess`).",
"Fundamentals_Setup_07_E": "Query and print the device's UUID (Universally Unique ID).",
"Fundamentals_Setup_07_F": "Query the device property `kernelExecTimeoutEnabled` and print whether a kernel execution timeout is enabled.",
"Fundamentals_Setup_07_G": "Print the `major` and `minor` compute capability numbers for the primary device.",
"Fundamentals_Setup_07_H": "Check if the device supports cooperative launch (`cooperativeLaunch`) and print the result.",
"Fundamentals_Setup_07_I": "Check if the device supports peer-to-peer access with other GPUs (`p2pAccessSupported`).",
"Fundamentals_Setup_07_J": "Check if the device is part of a TCC (Tesla Compute Cluster) (`isTCCDriver`).",
"Fundamentals_Setup_08": "Write a program that prints the CUDA driver version and the CUDA runtime version detected on your system. Use `cudaDriverGetVersion` and `cudaRuntimeGetVersion` and explain in a comment what the difference between the two is.",
"Fundamentals_Setup_08_A": "Write a program that gets the driver version and prints its major and minor versions separately. (e.g., Driver version 11.4 -> Major: 11, Minor: 4).",
"Fundamentals_Setup_08_B": "Write a program that gets the runtime version and prints its major and minor versions separately.",
"Fundamentals_Setup_08_C": "Write a program that compares the driver and runtime versions and prints a warning if the driver version is older than the runtime version, as this is an invalid configuration.",
"Fundamentals_Setup_08_D": "Explain in a comment where the CUDA driver is typically installed (e.g., as part of the NVIDIA display driver) and where the CUDA runtime is installed (e.g., with the CUDA Toolkit).",
"Fundamentals_Setup_08_E": "Use `nvcc --version` on the command line and compare its output to what `cudaRuntimeGetVersion` reports in your program.",
"Fundamentals_Setup_08_F": "Query and print the number of asynchronous engines the device has (`asyncEngineCount`).",
"Fundamentals_Setup_08_G": "Check if the device is a multi-GPU board (`isMultiGpuBoard`).",
"Fundamentals_Setup_08_H": "Print the total constant memory on the device in bytes.",
"Fundamentals_Setup_08_I": "Get the name of the current device and the length of the name string.",
"Fundamentals_Setup_08_J": "Use `cudaGetDeviceProperties` and print the `driverVersion` field from the properties struct.",
"Fundamentals_Setup_09": "Write a program to check and print whether the primary GPU supports running multiple kernels concurrently. The property you are looking for is `concurrentKernels` in `cudaDeviceProp`.",
"Fundamentals_Setup_09_A": "Add a comment to your program explaining what 'concurrent kernel execution' means and how it can improve performance.",
"Fundamentals_Setup_09_B": "Query and print the `asyncEngineCount` property, which indicates the number of engines available for overlapping data copies and kernel execution.",
"Fundamentals_Setup_09_C": "Check and print if the device can overlap a `memcpy` operation with kernel execution (`deviceOverlap`).",
"Fundamentals_Setup_09_D": "Write a conditional print statement: if `concurrentKernels` is true, print 'Device supports concurrency.', otherwise print 'Device does not support concurrency.'",
"Fundamentals_Setup_09_E": "Iterate through all devices and print the `concurrentKernels` status for each one.",
"Fundamentals_Setup_09_F": "Check if the device supports stream priorities (`streamPrioritiesSupported`).",
"Fundamentals_Setup_09_G": "Check if the device supports global L1 cache (`globalL1CacheSupported`).",
"Fundamentals_Setup_09_H": "Check if the device supports local L1 cache (`localL1CacheSupported`).",
"Fundamentals_Setup_09_I": "Print the amount of shared memory available per multiprocessor.",
"Fundamentals_Setup_09_J": "Print the maximum number of resident blocks per multiprocessor.",
"Fundamentals_Setup_10": "Write a program to check and print the L2 cache size of the primary GPU in kilobytes (KB).",
"Fundamentals_Setup_10_A": "Modify the program to print the L2 cache size in megabytes (MB) with one decimal place.",
"Fundamentals_Setup_10_B": "Iterate through all available GPUs and print the L2 cache size for each.",
"Fundamentals_Setup_10_C": "Add a comment explaining the role of the L2 cache in a GPU's memory hierarchy.",
"Fundamentals_Setup_10_D": "Query and print the `persistingL2CacheMaxSize` property.",
"Fundamentals_Setup_10_E": "Query and print the maximum number of threads per multiprocessor.",
"Fundamentals_Setup_10_F": "Query and print the memory bus width in bits.",
"Fundamentals_Setup_10_G": "Query and print the `regsPerBlock` property.",
"Fundamentals_Setup_10_H": "Query and print the GPU's clock rate in GHz.",
"Fundamentals_Setup_10_I": "Write a function `bool is_compute_capable(int major, int minor)` that queries the primary device and returns true if its compute capability is greater than or equal to the specified version.",
"Fundamentals_Setup_10_J": "Query and print the maximum texture dimension for a 1D texture using `cudaDeviceGetAttribute` with `cudaDevAttrMaxTexture1DWidth`.",
"Fundamentals_Setup_11": "Write a simple `__global__` function (a kernel) that does nothing. In your `main` function, attempt to compile this file using the NVIDIA CUDA Compiler (NVCC) command line, for example: `nvcc my_program.cu -o my_program`. Run the resulting executable.",
"Fundamentals_Setup_11_A": "Modify the program to print 'Hello from host!' before the kernel launch and 'Kernel finished!' after the kernel launch.",
"Fundamentals_Setup_11_B": "Launch the empty kernel with a configuration of 1 block and 1 thread.",
"Fundamentals_Setup_11_C": "Launch the empty kernel with a configuration of 16 blocks and 64 threads each.",
"Fundamentals_Setup_11_D": "Create a second empty kernel with a different name and launch both kernels sequentially from `main`.",
"Fundamentals_Setup_11_E": "Add `cudaDeviceSynchronize()` after the kernel launch and explain in a comment why this is important for timing and error checking.",
"Fundamentals_Setup_11_F": "Write a comment in your code showing the full `nvcc` command used to compile it.",
"Fundamentals_Setup_11_G": "Define `dim3` variables for the grid and block sizes and use them in the kernel launch configuration.",
"Fundamentals_Setup_11_H": "Write the kernel definition in a separate `.cu` file and the `main` function in another. Try to compile and link them together with `nvcc`.",
"Fundamentals_Setup_11_I": "Add the `-arch=sm_XX` flag to your `nvcc` command line to compile for a specific compute capability (e.g., `sm_75`).",
"Fundamentals_Setup_11_J": "Create a simple Makefile to automate the compilation process.",
"Fundamentals_Setup_12": "Create a CUDA program that deliberately fails to compile due to a syntax error within the `__global__` function (e.g., missing a semicolon). Compile it with `nvcc` and carefully analyze the error message produced. This helps in learning how to debug compilation issues.",
"Fundamentals_Setup_12_A": "Create a syntax error by misspelling `__global__` as `__globol__` and observe the compiler error.",
"Fundamentals_Setup_12_B": "Create an error by calling a regular C++ function (that is not marked `__device__`) from within your kernel.",
"Fundamentals_Setup_12_C": "Create an error by forgetting the `<<<...>>>` syntax for the kernel launch.",
"Fundamentals_Setup_12_D": "Define a kernel that takes an `int*` argument, but try to launch it without providing any arguments.",
"Fundamentals_Setup_12_E": "Try to declare a non-const static variable inside a kernel and observe the compilation error.",
"Fundamentals_Setup_12_F": "In `main`, try to call the kernel like a regular function `my_kernel()` instead of launching it with `<<<...>>>`.",
"Fundamentals_Setup_12_G": "Pass a host pointer (e.g., from `malloc`) directly to a kernel that expects a device pointer.",
"Fundamentals_Setup_12_H": "Deliberately mismatch the type of an argument, e.g., pass a `float` where the kernel expects an `int*`.",
"Fundamentals_Setup_12_I": "Try to use a C++ feature not supported by the targeted CUDA version (e.g., advanced templates or C++17 features without proper flags).",
"Fundamentals_Setup_12_J": "Compile with the `-v` (verbose) flag to see the internal steps `nvcc` takes, including where it separates host and device code.",
"Fundamentals_Setup_13": "Write a program that queries and prints the clock rate of the primary GPU in kilohertz (kHz).",
"Fundamentals_Setup_13_A": "Convert and print the clock rate in megahertz (MHz).",
"Fundamentals_Setup_13_B": "Convert and print the clock rate in gigahertz (GHz).",
"Fundamentals_Setup_13_C": "Iterate through all devices and print the clock rate for each one.",
"Fundamentals_Setup_13_D": "Query and print the memory clock rate of the primary GPU.",
"Fundamentals_Setup_13_E": "Query and print the device's multiprocessor count.",
"Fundamentals_Setup_13_F": "Calculate and print the theoretical floating-point operations per second (FLOPS) for single-precision on your GPU. A rough estimate is `ClockRate_GHz * MultiProcessorCount * CoresPerSM * 2`. You'll need to look up the cores per SM for your GPU architecture.",
"Fundamentals_Setup_13_G": "Query and print whether a kernel execution timeout is enabled on the device.",
"Fundamentals_Setup_13_H": "Query and print the PCI domain ID for the device.",
"Fundamentals_Setup_13_I": "Check if the GPU is part of a multi-GPU board and print the result.",
"Fundamentals_Setup_13_J": "Write a function `float get_clock_rate_ghz(int device_id)` that returns the clock rate in GHz for a given device.",
"Fundamentals_Setup_14": "Write a program to check if the primary GPU can overlap data copying with kernel execution. This capability is crucial for performance and is indicated by the `asyncEngineCount` property (a value > 0).",
"Fundamentals_Setup_14_A": "A more direct property is `deviceOverlap`. Query and print this boolean property for the primary device.",
"Fundamentals_Setup_14_B": "Based on the `asyncEngineCount`, print a message like 'Device has X copy engines.' If the count is greater than 0, also print 'Data copy can be overlapped with kernel execution.'",
"Fundamentals_Setup_14_C": "Iterate through all devices on the system and print the `asyncEngineCount` for each.",
"Fundamentals_Setup_14_D": "Add a comment explaining what a CUDA stream is and how it relates to overlapping operations.",
"Fundamentals_Setup_14_E": "Query and print the `concurrentKernels` property.",
"Fundamentals_Setup_14_F": "Query and print whether the device supports stream priorities.",
"Fundamentals_Setup_14_G": "Check if the device can map host memory using the `canMapHostMemory` property and print the result.",
"Fundamentals_Setup_14_H": "Check if the device supports host-mapped pageable memory access (`pageableMemoryAccess`).",
"Fundamentals_Setup_14_I": "Write a short paragraph in comments explaining the benefits of overlapping data transfers and computation.",
"Fundamentals_Setup_14_J": "Query and print the maximum pitch in bytes for a memory allocation.",
"Fundamentals_Setup_15": "Write a program that resets the primary CUDA device using `cudaDeviceReset()`. Explain in a comment why and when this function might be useful (e.g., for cleaning up resources or profiling).",
"Fundamentals_Setup_15_A": "Write a program that allocates memory on the GPU, then calls `cudaDeviceReset()`, and then attempts to use or free that memory. Observe the error that occurs.",
"Fundamentals_Setup_15_B": "Call `cudaDeviceReset()` at the very beginning of your `main` function to ensure a clean state.",
"Fundamentals_Setup_15_C": "Add comments explaining that `cudaDeviceReset()` destroys all allocations and contexts on the current device.",
"Fundamentals_Setup_15_D": "Write a program with multiple threads, where each thread sets a different GPU device. Have one thread call `cudaDeviceReset()` and discuss the potential implications for the other threads.",
"Fundamentals_Setup_15_E": "Use `nvidia-smi` to monitor GPU memory. Run a program that allocates memory but doesn't free it. Then run a program that allocates memory and then calls `cudaDeviceReset()`. Observe how `nvidia-smi` shows the memory being reclaimed in the second case.",
"Fundamentals_Setup_15_F": "Call `cudaDeviceSynchronize()` before `cudaDeviceReset()` and explain why this can be good practice.",
"Fundamentals_Setup_15_G": "Is `cudaDeviceReset()` necessary for a program that cleans up all its resources properly with `cudaFree`? Explain in a comment.",
"Fundamentals_Setup_15_H": "Write a loop that iterates through all available devices and calls `cudaSetDevice` followed by `cudaDeviceReset` for each one.",
"Fundamentals_Setup_15_I": "Explain the difference between `cudaDeviceReset()` and `cudaDeviceSynchronize()` in comments.",
"Fundamentals_Setup_15_J": "Create a C++ class with a destructor that calls `cudaDeviceReset()`. Create an instance of this class in `main`.",
"Fundamentals_Kernel_01": "Write a 'Hello World' CUDA program. The kernel should be launched with a single block containing a single thread. The host code (the `main` function) should print 'Kernel launched successfully!' after the kernel launch. The kernel itself will be empty.",
"Fundamentals_Kernel_01_A": "Modify the program to launch the kernel with two blocks, each with one thread.",
"Fundamentals_Kernel_01_B": "Modify the program to launch the kernel with one block containing two threads.",
"Fundamentals_Kernel_01_C": "Use `dim3` variables to define a grid of (4,1,1) and a block of (1,1,1) and launch the kernel.",
"Fundamentals_Kernel_01_D": "Use `dim3` variables to define a grid of (1,1,1) and a block of (64,1,1) and launch the kernel.",
"Fundamentals_Kernel_01_E": "Add `cudaDeviceSynchronize()` after the launch and print 'Kernel synchronized!' after it.",
"Fundamentals_Kernel_01_F": "Create a second empty kernel with a different name and launch both kernels from the host.",
"Fundamentals_Kernel_01_G": "Write a helper function `void launch_empty_kernel()` that contains the kernel launch call.",
"Fundamentals_Kernel_01_H": "Pass an integer literal (e.g., 5) as an argument to the empty kernel.",
"Fundamentals_Kernel_01_I": "Pass a float literal (e.g., 3.14f) as an argument to the empty kernel.",
"Fundamentals_Kernel_01_J": "Time the duration of the empty kernel launch using `cudaEvent` timers.",
"Fundamentals_Kernel_02": "Modify the previous 'Hello World' kernel to print a message from the GPU itself. The single thread in the kernel should use `printf` to print 'Hello from the GPU!'. Note that the output from the GPU might appear at a different time than host-side prints.",
"Fundamentals_Kernel_02_A": "Launch the printing kernel with one block of 10 threads. Observe the output.",
"Fundamentals_Kernel_02_B": "Launch the printing kernel with 10 blocks of one thread. Observe the output.",
"Fundamentals_Kernel_02_C": "Modify the `printf` to include the thread's ID: `printf(\"Hello from thread %d!\\n\", threadIdx.x);`. Launch with 8 threads.",
"Fundamentals_Kernel_02_D": "Modify the `printf` to include the block's ID: `printf(\"Hello from block %d!\\n\", blockIdx.x);`. Launch with 8 blocks.",
"Fundamentals_Kernel_02_E": "Pass an integer as an argument to the kernel and print its value from the device.",
"Fundamentals_Kernel_02_F": "Add host-side `printf` statements before and after the kernel launch to observe the non-deterministic ordering of host and device prints.",
"Fundamentals_Kernel_02_G": "Write a kernel where only thread 0 of a block prints a message (`if (threadIdx.x == 0) { ... }`). Launch with 128 threads.",
"Fundamentals_Kernel_02_H": "Print the values of `blockDim.x` and `gridDim.x` from within the kernel.",
"Fundamentals_Kernel_02_I": "Call `cudaDeviceSynchronize()` after the kernel launch. Does this guarantee the `printf` output will appear before subsequent host code executes? Explain in a comment. (Answer: No, `printf` is buffered).",
"Fundamentals_Kernel_02_J": "Pass a string (char*) from the host to the kernel and try to print it. Observe what happens and why it doesn't work as expected. (Hint: pointer points to host memory).",
"Fundamentals_Kernel_03": "Write a kernel that is launched with a 1D block of 64 threads. Each thread should print its own thread index within the block, which is accessible via `threadIdx.x`.",
"Fundamentals_Kernel_03_A": "Also print the block's dimension, `blockDim.x`, alongside the thread index.",
"Fundamentals_Kernel_03_B": "Modify the program to launch with 128 threads instead of 64.",
"Fundamentals_Kernel_03_C": "Modify the kernel so each thread prints `blockDim.x - 1 - threadIdx.x` (its index from the end of the block).",
"Fundamentals_Kernel_03_D": "Modify the kernel so that only threads with an even index print their ID.",
"Fundamentals_Kernel_03_E": "Modify the kernel so that only threads in the first half of the block (i.e., `threadIdx.x < blockDim.x / 2`) print their ID.",
"Fundamentals_Kernel_03_F": "Launch the kernel with a 2D block of (8, 8) threads and have each thread print only its `threadIdx.x`.",
"Fundamentals_Kernel_03_G": "Launch with a 1D block of 32 threads. Have each thread print its warp ID (`threadIdx.x / 32`).",
"Fundamentals_Kernel_03_H": "Launch with a 1D block of 64 threads. Have each thread print its lane ID within its warp (`threadIdx.x % 32`).",
"Fundamentals_Kernel_03_I": "Launch with one block of N threads. Pass an integer array `int* out` to the kernel. Have each thread write its `threadIdx.x` to `out[threadIdx.x]`.",
"Fundamentals_Kernel_03_J": "Launch with one block of 32 threads (one warp). Have thread 0 print a message. Then have thread 15 print a message. Then thread 31.",
"Fundamentals_Kernel_04": "Write a kernel that is launched with a 1D grid of 8 blocks, where each block has only one thread. Each thread should print its own block index, which is accessible via `blockIdx.x`.",
"Fundamentals_Kernel_04_A": "Also print the grid's dimension, `gridDim.x`, alongside the block index.",
"Fundamentals_Kernel_04_B": "Modify the program to launch with 32 blocks instead of 8.",
"Fundamentals_Kernel_04_C": "Modify the kernel so each thread prints `gridDim.x - 1 - blockIdx.x` (its block index from the end of the grid).",
"Fundamentals_Kernel_04_D": "Modify the kernel so that only blocks with an odd index print their ID.",
"Fundamentals_Kernel_04_E": "Launch the kernel with a 2D grid of (4, 4) blocks (each with one thread) and have each thread print only its `blockIdx.x`.",
"Fundamentals_Kernel_04_F": "Launch with a 1D grid of 8 blocks, but give each block 4 threads. Have only thread 0 of each block print the `blockIdx.x`.",
"Fundamentals_Kernel_04_G": "Launch with a 3D grid of (2, 2, 2) blocks, each with one thread. Have each thread print its `blockIdx.x`, `blockIdx.y`, and `blockIdx.z`.",
"Fundamentals_Kernel_04_H": "Launch with a grid of N blocks. Pass an integer array `int* out`. Have each thread write its `blockIdx.x` to `out[blockIdx.x]`.",
"Fundamentals_Kernel_04_I": "Launch with 16 blocks. Have each thread print its block ID multiplied by 10.",
"Fundamentals_Kernel_04_J": "Pass an offset value to the kernel. Have each thread print `blockIdx.x + offset`.",
"Fundamentals_Kernel_05": "Combine the concepts of the previous two questions. Launch a kernel with a grid of 4 blocks, each containing 16 threads. Each thread must calculate and print its unique 'global ID' using the formula: `int id = blockIdx.x * blockDim.x + threadIdx.x;`.",
"Fundamentals_Kernel_05_A": "Change the launch configuration to 8 blocks of 32 threads and verify the global IDs are still unique and correct.",
"Fundamentals_Kernel_05_B": "Also print the thread's `blockIdx.x` and `threadIdx.x` next to its calculated global ID.",
"Fundamentals_Kernel_05_C": "Calculate the total number of threads on the host (`gridDim.x * blockDim.x`) and pass it to the kernel. Inside the kernel, print the global ID and the total number of threads.",
"Fundamentals_Kernel_05_D": "Modify the formula for a 2D grid of 1D blocks. Launch with a grid of (4, 2) blocks, each with 16 threads. The global ID formula is more complex. Start by calculating a unique block ID: `int blockId = blockIdx.y * gridDim.x + blockIdx.x;`. Then calculate the global thread ID.",
"Fundamentals_Kernel_05_E": "Write the global ID to an output array at the corresponding index: `output[id] = id;`. Verify the result on the host.",
"Fundamentals_Kernel_05_F": "Launch with more threads than you need (e.g., for an array of size 100, launch 128 threads). Add a check `if (id < 100)` before printing or writing to memory.",
"Fundamentals_Kernel_05_G": "Write a kernel that calculates the global ID and then prints whether it is even or odd.",
"Fundamentals_Kernel_05_H": "Pass two arrays, `in_a` and `in_b`, and one output array `out_c`. Have each thread with global ID `id` compute `out_c[id] = in_a[id] + in_b[id]`.",
"Fundamentals_Kernel_05_I": "Calculate the global ID starting from 1 instead of 0.",
"Fundamentals_Kernel_05_J": "Explain in a comment why this formula for global ID works.",
"Fundamentals_Kernel_06": "Write a kernel that accepts an integer pointer `int* data` as an argument. The host code should allocate a single integer on the GPU (`cudaMalloc`), launch the kernel with one thread, and pass the pointer. The kernel should write the value `1337` to the memory location pointed to by `data`. Finally, the host should copy the value back (`cudaMemcpy`) and print it to verify.",
"Fundamentals_Kernel_06_A": "Modify the program to allocate space for 10 integers. Have the kernel write `1337` to the 5th element (`data[4]`). Verify on the host.",
"Fundamentals_Kernel_06_B": "Modify the kernel to write the value of its `threadIdx.x` to `data[0]`. Launch with one thread and verify.",
"Fundamentals_Kernel_06_C": "Modify the kernel to increment the value at `data[0]`. The host should initialize the value to 100, copy it to the device, launch the kernel to increment it, copy it back, and verify the result is 101.",
"Fundamentals_Kernel_06_D": "Use a `float*` instead of an `int*` and write the value `3.14f`.",
"Fundamentals_Kernel_06_E": "Write a kernel that takes two pointers, `int* in` and `int* out`. It should read the value from `in`, multiply it by 2, and write the result to `out`.",
"Fundamentals_Kernel_06_F": "Wrap the memory allocation and deallocation in a C++ class using RAII (Constructor allocates, Destructor frees).",
"Fundamentals_Kernel_06_G": "Launch the kernel with 10 threads, but have all 10 threads try to write to `data[0]`. What value do you expect to see when you copy it back? (This is a race condition).",
"Fundamentals_Kernel_06_H": "Allocate memory, but forget to copy the pointer's value back from the device. Print the host variable. What does it show and why?",
"Fundamentals_Kernel_06_I": "Allocate memory, copy a value to it, but forget to launch the kernel. Copy the value back. Does it match?",
"Fundamentals_Kernel_06_J": "Add robust error checking around every CUDA API call (`cudaMalloc`, `cudaMemcpy`, `cudaFree`).",
"Fundamentals_Kernel_07": "Launch a kernel with a 2D block of threads, for example, 8x8 threads. Each thread should print its 2D thread index (`threadIdx.x`, `threadIdx.y`).",
"Fundamentals_Kernel_07_A": "Change the launch configuration to a non-square 2D block, like 16x4 threads, and print the 2D indices.",
"Fundamentals_Kernel_07_B": "Inside the kernel, calculate a flattened 1D thread index from the 2D indices: `int id_1d = threadIdx.y * blockDim.x + threadIdx.x;`. Print this value.",
"Fundamentals_Kernel_07_C": "Also print the block's 2D dimensions (`blockDim.x`, `blockDim.y`).",
"Fundamentals_Kernel_07_D": "Launch a 3D block of (4, 4, 4) threads. Have each thread print its 3D thread index (`threadIdx.x`, `threadIdx.y`, `threadIdx.z`).",
"Fundamentals_Kernel_07_E": "Launch an 8x8 block. Have only the thread at (0, 0) print a message.",
"Fundamentals_Kernel_07_F": "Launch an 8x8 block. Have only the threads on the first row (`threadIdx.y == 0`) print their `threadIdx.x`.",
"Fundamentals_Kernel_07_G": "Launch an 8x8 block. Pass a 2D array (flattened to 1D) `int* data`. Have each thread write its flattened 1D index to the corresponding location in the array.",
"Fundamentals_Kernel_07_H": "Define the block dimensions using a `dim3` variable: `dim3 block_dim(8, 8);`.",
"Fundamentals_Kernel_07_I": "Modify the kernel so that threads with `threadIdx.x == threadIdx.y` (on the diagonal) print a special message.",
"Fundamentals_Kernel_07_J": "Pass an integer offset `d`. Have threads print `(threadIdx.x + d, threadIdx.y + d)`.",
"Fundamentals_Kernel_08": "Launch a kernel with a 2D grid of blocks, for example, 4x4 blocks, each with a single thread. Each thread should print its 2D block index (`blockIdx.x`, `blockIdx.y`).",
"Fundamentals_Kernel_08_A": "Change the launch configuration to a non-square 2D grid, like 8x2 blocks, and print the 2D indices.",
"Fundamentals_Kernel_08_B": "Inside the kernel, calculate a flattened 1D block index from the 2D indices: `int id_1d = blockIdx.y * gridDim.x + blockIdx.x;`. Print this value.",
"Fundamentals_Kernel_08_C": "Also print the grid's 2D dimensions (`gridDim.x`, `gridDim.y`).",
"Fundamentals_Kernel_08_D": "Launch a 3D grid of (2, 2, 2) blocks. Have each thread print its 3D block index (`blockIdx.x`, `blockIdx.y`, `blockIdx.z`).",
"Fundamentals_Kernel_08_E": "Combine with 2D blocks. Launch a 2x2 grid of 4x4 blocks. Have each thread calculate and print its global 2D index: `int global_x = blockIdx.x * blockDim.x + threadIdx.x;` and `int global_y = ...`.",
"Fundamentals_Kernel_08_F": "Launch a 4x4 grid. Have only the block at (0, 0) print a message.",
"Fundamentals_Kernel_08_G": "Launch a 4x4 grid. Pass a 2D array (flattened to 1D) `int* data`. Have each thread write its flattened 1D block index to the corresponding location in the array.",
"Fundamentals_Kernel_08_H": "Define the grid dimensions using a `dim3` variable: `dim3 grid_dim(4, 4);`.",
"Fundamentals_Kernel_08_I": "Modify the kernel so that blocks with `blockIdx.x == blockIdx.y` (on the diagonal) print a special message.",
"Fundamentals_Kernel_08_J": "Launch with a 4x4 grid of 8x8 blocks. Each thread should write its global flattened ID to a 2D output matrix.",
"Fundamentals_Kernel_09": "Create a `__device__` function called `square_me` that takes an integer `x` and returns `x*x`. Create a `__global__` kernel that calls this `square_me` function on its `threadIdx.x` and prints the result.",
"Fundamentals_Kernel_09_A": "Create a `__device__` function `add_two(int a, int b)` that returns `a+b`. Call it from your kernel.",
"Fundamentals_Kernel_09_B": "Create a `__device__` function `void increment(int* x)` that increments the value pointed to by x. The kernel should call this on an element of an array in device memory.",
"Fundamentals_Kernel_09_C": "Explain the difference between `__global__` and `__device__` functions in a comment.",
"Fundamentals_Kernel_09_D": "Can you call a `__global__` function from a `__device__` function? (No, unless using Dynamic Parallelism). Can you call a `__device__` function from a `__global__` function? (Yes). Explain this in a comment.",
"Fundamentals_Kernel_09_E": "Create a `__host__ __device__` function (a function that can be called from both host and device). Call it from both `main` and your kernel.",
"Fundamentals_Kernel_09_F": "Place your `__device__` function definition after the `__global__` kernel that calls it. Does it compile? (It might require a forward declaration).",
"Fundamentals_Kernel_09_G": "Create a `__device__` function that is recursive, like a factorial function. Try calling it from a kernel. Be aware of stack limitations.",
"Fundamentals_Kernel_09_H": "Create a `__device__` function that takes a pointer as an argument.",
"Fundamentals_Kernel_09_I": "Create a header file (`my_device_functions.cuh`) and put your `__device__` function in it. Include this header in your main `.cu` file.",
"Fundamentals_Kernel_09_J": "Write a `__device__` function that calculates the distance between two 2D points, `sqrt((x2-x1)^2 + (y2-y1)^2)`.",
"Fundamentals_Kernel_10": "Define a simple C struct on the host. Pass an instance of this struct by value to a kernel. The kernel should print one of the struct's members to verify it was received correctly.",
"Fundamentals_Kernel_10_A": "Define a struct `MyData { int a; float b; }`. In the host, initialize it with `{10, 3.14f}`. Pass it to the kernel and have the kernel print both members.",
"Fundamentals_Kernel_10_B": "Modify the kernel to change a member of the struct it received. Does this change affect the original struct on the host? Explain why or why not (pass-by-value).",
"Fundamentals_Kernel_10_C": "Pass an array of these structs from host to device. Have each thread `i` access the `i`-th element of the struct array and print a member.",
"Fundamentals_Kernel_10_D": "Define a nested struct (a struct that contains another struct as a member). Pass it by value to a kernel and access a member of the inner struct.",
"Fundamentals_Kernel_10_E": "Define a struct that contains a small, fixed-size array (e.g., `float vec[3]`). Pass it by value and have the kernel print the array elements.",
"Fundamentals_Kernel_10_F": "What happens if the struct contains a pointer member (e.g., `int* p`)? Pass it by value, and have the kernel try to dereference the pointer. Explain the result.",
"Fundamentals_Kernel_10_G": "Compare passing a struct with 3 floats vs. passing 3 separate float arguments to a kernel. Is there a performance difference? (Likely negligible, but good to think about).",
"Fundamentals_Kernel_10_H": "Create a `__device__` function that takes the struct as an argument by value.",
"Fundamentals_Kernel_10_I": "Use `sizeof()` on the host and device to print the size of the struct and verify they are the same.",
"Fundamentals_Kernel_10_J": "Pass the struct by constant reference (`const MyData&`) to the kernel. This can be more efficient for large structs.",
"Fundamentals_Kernel_11": "Pass a pointer to a struct to a kernel. The host should allocate the struct on both host and device. Initialize it on the host, copy to device. The kernel should modify a member of the struct. The host should copy it back and verify the modification.",
"Fundamentals_Kernel_11_A": "The kernel receives `MyData* d_data`. Modify it to read a value: `int x = d_data->a;` and then modify another value: `d_data->b = 2.0f * x;`.",
"Fundamentals_Kernel_11_B": "Allocate an array of N structs on the device. Launch a kernel with N threads. Each thread `i` should modify the struct at `d_data[i]`.",
"Fundamentals_Kernel_11_C": "In the host, allocate the struct using `cudaMallocManaged` instead of `cudaMalloc`. Initialize on host, launch kernel to modify, synchronize, and then access directly on host to verify.",
"Fundamentals_Kernel_11_D": "Create a `__device__` function that takes a pointer to the struct (`MyData*`) and performs an operation on it.",
"Fundamentals_Kernel_11_E": "From the host, use `cudaMemset` to zero out the struct's memory on the device after allocating it.",
"Fundamentals_Kernel_11_F": "Pass the struct pointer, but forget to copy the data from host to device first. What values does the kernel see when it reads from the struct's members?",
"Fundamentals_Kernel_11_G": "Pass the struct pointer, have the kernel modify it, but forget to copy the data back from device to host. Does the verification on the host pass?",
"Fundamentals_Kernel_11_H": "In a kernel with many threads, have every thread try to modify the same member of the single struct instance (e.g., `d_data->a = threadIdx.x;`). What is the final value after copying back? (Race condition).",
"Fundamentals_Kernel_11_I": "Use `atomicAdd` to safely have multiple threads increment a counter member within the single shared struct instance.",
"Fundamentals_Kernel_11_J": "Define a struct with many members. Time the difference between copying the whole struct vs. using `cudaMemcpy` on individual members (the former is much better).",
"Fundamentals_Kernel_12": "Store the grid and block dimensions in `dim3` variables on the host before launching the kernel. Launch the kernel using these variables, e.g., `myKernel<<<gridDim, blockDim>>>();`. This is the standard way to configure a launch.",
"Fundamentals_Kernel_12_A": "Set up a 1D problem. Define `int N = 1024; int threads_per_block = 256;`. Calculate the number of blocks needed: `int blocks_per_grid = (N + threads_per_block - 1) / threads_per_block;`. Create `dim3` variables from this and launch.",
"Fundamentals_Kernel_12_B": "Set up a 2D problem for a 512x512 image. Define a 2D block size of 16x16. Calculate the 2D grid dimensions needed. Create `dim3` variables for both grid and block and launch the kernel.",
"Fundamentals_Kernel_12_C": "Create a helper C++ function `void launch_kernel_1d(int n_elements)` that computes the correct grid/block dimensions and launches a kernel.",
"Fundamentals_Kernel_12_D": "Declare `dim3 grid(4);` and `dim3 block(64);`. Launch a kernel and print `gridDim.x` and `blockDim.x` from within the kernel to verify they match.",
"Fundamentals_Kernel_12_E": "Declare `dim3 grid(2, 2);` and `dim3 block(8, 8);`. Launch a kernel and print the `x` and `y` components of `gridDim` and `blockDim`.",
"Fundamentals_Kernel_12_F": "Try to create a `dim3` block variable with a size larger than the device's `maxThreadsPerBlock` limit (e.g., `dim3 block(2048)`). What happens at kernel launch?",
"Fundamentals_Kernel_12_G": "Read the desired number of threads from the command line and dynamically calculate the grid/block `dim3` variables at runtime.",
"Fundamentals_Kernel_12_H": "A kernel can optionally take shared memory size and a stream as launch parameters: `kernel<<<grid, block, shared_mem_bytes, stream>>>();`. Practice by setting the shared memory size to 0 and the stream to `0` (default stream).",
"Fundamentals_Kernel_12_I": "Initialize a `dim3` variable with all three dimensions, e.g., `dim3 grid(4, 2, 2);`.",
"Fundamentals_Kernel_12_J": "Explain in a comment why calculating the number of blocks as `(N + BS - 1) / BS` is better than `N / BS`.",
"Fundamentals_Kernel_13": "Write a kernel that accepts two integer pointers, `a` and `b`. The kernel (with a single thread) should read the value from `a`, add 10 to it, and write the result to `b`. The host must allocate, initialize `a`, and verify the result in `b`.",
"Fundamentals_Kernel_13_A": "Generalize the kernel to work on arrays. It should take `int* a`, `int* b`, and `int N`. Each thread `i` should compute `b[i] = a[i] + 10;`.",
"Fundamentals_Kernel_13_B": "Write a kernel that takes three pointers, `a`, `b`, and `c`, and computes `c[i] = a[i] + b[i]`.",
"Fundamentals_Kernel_13_C": "Write a kernel that performs the operation in-place. It takes a single pointer `a` and computes `a[i] = a[i] + 10;`.",
"Fundamentals_Kernel_13_D": "Modify the kernel to take a fourth argument, `int scalar`, and compute `b[i] = a[i] + scalar;`.",
"Fundamentals_Kernel_13_E": "Use `float` arrays instead of `int` arrays.",
"Fundamentals_Kernel_13_F": "Implement element-wise multiplication: `c[i] = a[i] * b[i]`.",
"Fundamentals_Kernel_13_G": "Implement a kernel that copies one device array to another: `b[i] = a[i]`.",
"Fundamentals_Kernel_13_H": "In the host code, use `cudaMemset` to initialize the input array `a` to a specific value (e.g., 1) on the device before launching the kernel.",
"Fundamentals_Kernel_13_I": "Write a kernel that swaps the values between two arrays: `int temp = a[i]; a[i] = b[i]; b[i] = temp;`.",
"Fundamentals_Kernel_13_J": "Chain kernel calls. The output of the first kernel (`b`) becomes the input to a second kernel that, for example, squares every element.",
"Fundamentals_Kernel_14": "Experiment with kernel launch failure. Try to launch a kernel with more threads per block than the device maximum (e.g., 2048). Use proper error checking after the launch to catch and report the `cudaErrorInvalidConfiguration` error.",
"Fundamentals_Kernel_14_A": "Trigger `cudaErrorInvalidConfiguration` by requesting more shared memory per block than is available on the device.",
"Fundamentals_Kernel_14_B": "Trigger `cudaErrorInvalidConfiguration` by using a 2D block size where one of the dimensions exceeds the device limit (e.g., `dim3 block(2048, 1, 1)`).",
"Fundamentals_Kernel_14_C": "Use `cudaPeekAtLastError()` immediately after the kernel launch to get the error code without waiting for a synchronization.",
"Fundamentals_Kernel_14_D": "Use `cudaDeviceSynchronize()` after the kernel launch. This is a common way to catch asynchronous errors from the kernel execution itself, not just launch configuration errors.",
"Fundamentals_Kernel_14_E": "Write a `CHECK_KERNEL_LAUNCH()` macro that calls `cudaPeekAtLastError()` and `cudaDeviceSynchronize()` and reports any errors.",
"Fundamentals_Kernel_14_F": "Launch a kernel with a grid dimension of 0. What error does this cause?",
"Fundamentals_Kernel_14_G": "Launch a kernel with a block dimension of 0. What error does this cause?",
"Fundamentals_Kernel_14_H": "Query the device for its `maxThreadsPerBlock` and then deliberately launch a kernel with `maxThreadsPerBlock + 1` threads to dynamically trigger the error.",
"Fundamentals_Kernel_14_I": "Explain the difference between a synchronous error (like `cudaMalloc` failing) and an asynchronous error (like a bad kernel launch configuration).",
"Fundamentals_Kernel_14_J": "Try to launch a kernel using a function pointer that is null. What error occurs?",
"Fundamentals_Kernel_15": "Write a kernel that does an infinite loop (`while(1);`). Launch it. Observe that your program hangs. This demonstrates the asynchronous nature of kernel launches and the importance of `cudaDeviceSynchronize` for debugging.",
"Fundamentals_Kernel_15_A": "Launch the hanging kernel and try to use `nvidia-smi` in another terminal. You should see the GPU utilization at 100%.",
"Fundamentals_Kernel_15_B": "On Linux, the OS may have a watchdog timer that kills the kernel after a few seconds, causing `cudaDeviceSynchronize` to return an error (`cudaErrorLaunchTimeout`). Try to catch and report this error.",
"Fundamentals_Kernel_15_C": "Modify the infinite loop kernel to take a pointer `int* flag`. The loop should be `while(*flag == 0);`. From the host, after a delay (`sleep(1)`), copy a value of `1` to the flag on the device to stop the kernel. (This demonstrates device-side polling).",
"Fundamentals_Kernel_15_D": "Write a kernel that has a very long but finite loop. Call it without `cudaDeviceSynchronize`. The host program finishes almost instantly. Now add `cudaDeviceSynchronize`. The host program now waits for the kernel to complete.",
"Fundamentals_Kernel_15_E": "Explain in a comment that because kernel launches are asynchronous, the CPU can continue working on other tasks while the GPU is busy.",
"Fundamentals_Kernel_15_F": "Launch the hanging kernel. Does the `main` function ever exit? Why or why not?",
"Fundamentals_Kernel_15_G": "Launch a normal, fast kernel, followed immediately by the hanging kernel. What is the program's behavior?",
"Fundamentals_Kernel_15_H": "Write a kernel with an infinite loop where only thread 0 loops forever. The other threads finish. Does the block as a whole ever finish? (No, requires all threads to finish).",
"Fundamentals_Kernel_15_I": "Can you stop a hanging kernel from the host code without resetting the device? (Generally no).",
"Fundamentals_Kernel_15_J": "Launch the infinite loop kernel on a non-default stream. Does it still hang the application when you call `cudaDeviceSynchronize()`?",
"Fundamentals_Memory_01": "Write a program that allocates an array of 256 integers on the host and initializes it with values from 0 to 255. Then, allocate memory for 256 integers on the GPU device using `cudaMalloc`. Finally, copy the host array to the device array using `cudaMemcpy` with the `cudaMemcpyHostToDevice` kind.",
"Fundamentals_Memory_01_A": "After copying, use `cudaFree` to release the device memory.",
"Fundamentals_Memory_01_B": "Change the data type from `int` to `float` and initialize the host array with floating-point values.",
"Fundamentals_Memory_01_C": "Write a helper function `void host_to_device_copy(int* h_data, int* d_data, int size)` that encapsulates the `cudaMemcpy` call.",
"Fundamentals_Memory_01_D": "Instead of `malloc`, use `new int[256]` for the host allocation.",
"Fundamentals_Memory_01_E": "Calculate the size in bytes required for the allocation (`256 * sizeof(int)`) and use that variable in both `cudaMalloc` and `cudaMemcpy`.",
"Fundamentals_Memory_01_F": "Try to `cudaMalloc` zero bytes. Does it return an error or a valid pointer?",
"Fundamentals_Memory_01_G": "Add error checking to the `cudaMalloc` and `cudaMemcpy` calls.",
"Fundamentals_Memory_01_H": "Allocate and copy a much larger array, e.g., 1 million integers.",
"Fundamentals_Memory_01_I": "After allocating with `cudaMalloc`, use `cudaMemset` to set all the bytes of the device array to 0 before copying the host data over.",
"Fundamentals_Memory_01_J": "Allocate on host, allocate on device, but forget the `cudaMemcpy` call. What is in the device memory?",
"Fundamentals_Memory_02": "Extend the previous program. After copying data to the device, create a second host array of 256 integers. Copy the data from the device array back to this new host array using `cudaMemcpy` with the `cudaMemcpyDeviceToHost` kind. Verify that every element in the new host array matches the original.",
"Fundamentals_Memory_02_A": "Write a loop that iterates through the arrays and prints an error message if a mismatch is found, then breaks.",
"Fundamentals_Memory_02_B": "Instead of a second host array, copy the data back into the original host array.",
"Fundamentals_Memory_02_C": "Create a `bool` flag `is_correct` and set it to `false` if any element mismatches. Print a final 'Verification successful' or 'Verification FAILED' message.",
"Fundamentals_Memory_02_D": "Copy only the first half of the array back from the device and verify it.",
"Fundamentals_Memory_02_E": "Copy a sub-section of the device array (e.g., elements 100 to 149) to the beginning of the host array. This requires pointer arithmetic on both host and device pointers.",
"Fundamentals_Memory_02_F": "Write a kernel that modifies the data on the device (e.g., doubles every element) before you copy it back. Verify the result is as expected.",
"Fundamentals_Memory_02_G": "Perform a round trip: HtoD, then DtoH. Time the entire process.",
"Fundamentals_Memory_02_H": "Use `memcmp` to verify the entire block of memory at once instead of a loop.",
"Fundamentals_there_02_I": "Deliberately corrupt one byte of the data on the host before verification to ensure your verification code works.",
"Fundamentals_Memory_02_J": "Write a function `void print_device_array(int* d_array, int size)` that allocates temporary host memory, copies from device, prints, and then frees the temporary memory.",
"Fundamentals_Memory_03": "Write a program that allocates two integer arrays, `d_a` and `d_b`, on the device. Initialize `d_a` by copying data from the host. Then, use `cudaMemcpy` with the `cudaMemcpyDeviceToDevice` kind to copy the contents of `d_a` directly to `d_b` without involving the host.",
"Fundamentals_Memory_03_A": "To verify the DtoD copy, copy `d_b` back to a host array and check its contents.",
"Fundamentals_Memory_03_B": "Time the `cudaMemcpyDeviceToDevice` operation. Compare this timing to a DtoH followed by an HtoD copy. The DtoD copy should be much faster.",
"Fundamentals_Memory_03_C": "Write a custom kernel `void copy_kernel(int* in, int* out, int N)` that performs the same copy operation as `cudaMemcpyDeviceToDevice`. Time this kernel and compare its performance to the API call.",
"Fundamentals_Memory_03_D": "Use `cudaMemset` to initialize `d_a` to a value (e.g., 5) instead of copying from the host. Then perform the DtoD copy.",
"Fundamentals_Memory_03_E": "Copy only a portion of `d_a` to `d_b` using pointer arithmetic.",
"Fundamentals_Memory_03_F": "Allocate `d_a` and `d_b`. Use `cudaMemcpy(d_b, h_a, size, cudaMemcpyHostToDevice)` to initialize `d_a`. What is the error? Change `h_a` to `d_a`. What is the error? Explain why the kind parameter is critical.",
"Fundamentals_Memory_03_G": "After a DtoD copy, modify `d_a`. Does `d_b` change? (No, it's a deep copy).",
"Fundamentals_Memory_03_H": "What happens if the source and destination pointers for a `cudaMemcpyDeviceToDevice` overlap? (Behavior is undefined, use `cudaMemmove` if needed).",
"Fundamentals_Memory_03_I": "Perform a three-way copy: HtoD to `d_a`, DtoD from `d_a` to `d_b`, DtoD from `d_b` to `d_c`. Verify `d_c`.",
"Fundamentals_Memory_03_J": "Write a generic `device_copy` function that takes two `void*` pointers, a size, and calls `cudaMemcpy` with the DtoD kind.",
"Fundamentals_Memory_04": "Write a function that takes an integer `N`, allocates an array of `N` floats on the device, and initializes all its elements to zero using `cudaMemset`. The function should return the device pointer.",
"Fundamentals_Memory_04_A": "Write a `main` function that calls your function, then copies the zeroed array back to the host and verifies that all elements are indeed 0.",
"Fundamentals_Memory_04_B": "Modify the function to use `cudaMemset` to initialize all bytes to `0xFF` (which corresponds to -1 for signed integers).",
"Fundamentals_Memory_04_C": "Write a kernel to achieve the same result: `kernel_set_zero(float* arr, int N)` where each thread sets one element to `0.0f`.",
"Fundamentals_Memory_04_D": "Compare the performance of `cudaMemset` vs. your custom zeroing kernel for a very large array. `cudaMemset` is usually highly optimized.",
"Fundamentals_Memory_04_E": "What is the limitation of `cudaMemset`? (It can only set every byte to the same value). Explain why you can't use it to initialize an array of floats to `1.0f`.",
"Fundamentals_Memory_04_F": "Write a function that takes a device pointer and a size and calls `cudaMemset` to zero it out.",
"Fundamentals_Memory_04_G": "Demonstrate that `cudaMemset` works on a 2D allocation from `cudaMallocPitch`.",
"Fundamentals_Memory_04_H": "Allocate memory and use `cudaMemsetAsync` with a non-default stream.",
"Fundamentals_Memory_04_I": "Add error checking to the `cudaMemset` call.",
"Fundamentals_Memory_04_J": "Pass a `nullptr` to `cudaMemset` and use error checking to catch the invalid value error.",
"Fundamentals_Memory_05": "Write a program that allocates a large amount of memory on the device (e.g., 512MB) and then immediately frees it using `cudaFree`. Use robust error checking for all API calls to ensure the allocation and deallocation were successful.",
"Fundamentals_Memory_05_A": "Modify the program to allocate and free the memory in a loop 100 times.",
"Fundamentals_Memory_05_B": "Try to call `cudaFree` on the same pointer twice. What happens? Use error checking to see the result of the second call.",
"Fundamentals_Memory_05_C": "Try to call `cudaFree` on a `nullptr`. Check the return code. Is it an error?",
"Fundamentals_Memory_05_D": "Try to call `cudaFree` on a host pointer from `malloc`. What error does this produce?",
"Fundamentals_Memory_05_E": "Use `nvidia-smi` to watch the GPU memory usage while your program runs. Step through with a debugger to see the memory increase after `cudaMalloc` and decrease after `cudaFree`.",
"Fundamentals_Memory_05_F": "Write a program that allocates 10 small chunks of memory and stores their pointers in an array. Then, loop through the array to free them all.",
"Fundamentals_Memory_05_G": "What happens if you allocate memory but your program crashes before `cudaFree` is called? Does the memory get reclaimed?",
"Fundamentals_Memory_05_H": "Write a simple C++ vector-like class for device memory. The constructor calls `cudaMalloc`, the destructor calls `cudaFree`.",
"Fundamentals_Memory_05_I": "Allocate memory, launch a kernel that uses it, synchronize, and then free the memory.",
"Fundamentals_Memory_05_J": "Forget to call `cudaFree`. Run the program. This is a memory leak.",
"Fundamentals_Memory_06": "Write a program that deliberately leaks memory on the device by allocating it but never calling `cudaFree`. Run the program multiple times and use the `nvidia-smi` command-line tool to observe how the GPU's memory usage increases.",
"Fundamentals_Memory_06_A": "Write the memory-leaking code inside a function. Call this function in a loop in `main` to leak memory more quickly.",
"Fundamentals_Memory_06_B": "Does the leaked memory get freed when the application exits? Use `nvidia-smi` to confirm.",
"Fundamentals_Memory_06_C": "Explain in a comment why memory leaks are bad, especially on a shared resource like a GPU.",
"Fundamentals_Memory_06_D": "Demonstrate a different kind of leak: allocate with `cudaMallocHost` (pinned memory) and forget to free with `cudaFreeHost`.",
"Fundamentals_Memory_06_E": "Use `cudaMemGetInfo` to query the amount of free and total memory on the device. Print these values before and after the deliberate leak.",
"Fundamentals_Memory_06_F": "Create a `std::vector` of device pointers. In a loop, allocate memory and `push_back` the pointer into the vector. Never free them.",
"Fundamentals_Memory_06_G": "Can tools like Valgrind detect CUDA device memory leaks? (Not directly, but `cuda-memcheck --leak-check` can).",
"Fundamentals_Memory_06_H": "In a loop, allocate memory and overwrite the pointer variable in the next iteration. Explain why this is a leak.",
"Fundamentals_Memory_06_I": "Run your leaky program with `cuda-memcheck --leak-check full` and analyze the output.",
"Fundamentals_Memory_06_J": "Write a comment block with best practices for avoiding memory leaks in CUDA (e.g., using RAII wrappers).",
"Fundamentals_Memory_07": "Write a program that tries to access a device pointer from host code (e.g., `int* d_ptr; cudaMalloc(&d_ptr, ...); printf(\"%d\", d_ptr[0]);`). Observe the segmentation fault or error this causes and explain why it happens.",
"Fundamentals_Memory_07_A": "Explain in a comment the concept of separate host and device memory address spaces.",
"Fundamentals_Memory_07_B": "Now do the reverse: allocate host memory with `malloc`, pass the host pointer to a kernel, and have the kernel try to dereference it. What happens? (This can cause an error or unspecified behavior).",
"Fundamentals_Memory_07_C": "Allocate memory with `cudaMallocManaged`. Now try to access this pointer from the host. Does it work? Why?",
"Fundamentals_Memory_07_D": "If you print the value of a device pointer (`printf(\"%p\", d_ptr);`), it prints an address. Explain that this address is only valid from the GPU's perspective.",
"Fundamentals_Memory_07_E": "Try to pass the address of a host variable (`int x; int* p = &x;`) to a kernel. Dereference it. Observe the error.",
"Fundamentals_Memory_07_F": "Write a `__host__` function that takes an `int*` and dereferences it. Call it with a device pointer. Observe the segfault.",
"Fundamentals_Memory_07_G": "Write a `__global__` function that takes an `int*` and dereferences it. Call it with a host pointer. Observe the error at runtime.",
"Fundamentals_Memory_07_H": "Explain how `cudaMemcpy` bridges the gap between these two separate memory spaces.",
"Fundamentals_Memory_07_I": "Try to use `memcpy` or `std::copy` with a device pointer. What happens?",
"Fundamentals_Memory_07_J": "Query the device property for `unifiedAddressing`. If true, explain what this means for host and device pointers.",
"Fundamentals_Memory_08": "Time the data transfer speed (bandwidth) of `cudaMemcpy`. Transfer a large array (e.g., 256MB) from host to device. Record the time before and after the copy using a CUDA event timer (`cudaEventCreate`, `cudaEventRecord`, `cudaEventSynchronize`, `cudaEventElapsedTime`). Calculate and print the bandwidth in GB/s.",
"Fundamentals_Memory_08_A": "Perform the same timing for a device-to-host copy. Is it typically faster, slower, or the same?",
"Fundamentals_Memory_08_B": "Perform the same timing for a device-to-device copy.",
"Fundamentals_Memory_08_C": "Create a loop that performs the HtoD copy multiple times and averages the bandwidth to get a more stable measurement.",
"Fundamentals_Memory_08_D": "Plot the measured bandwidth for different transfer sizes (e.g., 1KB, 1MB, 16MB, 128MB). Notice that bandwidth is lower for small transfers due to latency overhead.",
"Fundamentals_Memory_08_E": "The formula for bandwidth is `(bytes / 1e9) / (milliseconds / 1000.0)`. Implement this in your code.",
"Fundamentals_Memory_08_F": "Wrap the event timer logic in a C++ class `GpuTimer` for convenience.",
"Fundamentals_Memory_08_G": "Compare the timing from CUDA events to timing with `std::chrono::high_resolution_clock`. Explain why CUDA events are more accurate for timing GPU operations.",
"Fundamentals_Memory_08_H": "Ensure you call `cudaEventDestroy` to clean up the event handles.",
"Fundamentals_Memory_08_I": "Use `cudaMemcpyAsync` on a stream and record events around it to time the asynchronous transfer.",
"Fundamentals_Memory_08_J": "Compare your measured bandwidth to the theoretical peak bandwidth of your GPU's PCIe bus.",
"Fundamentals_Memory_09": "Compare the bandwidth of pageable vs. pinned (non-pageable) host memory. Perform the timing from the previous exercise. Then, repeat it but allocate the host memory using `cudaMallocHost` instead of `malloc`. Print both bandwidths and observe the difference.",
"Fundamentals_Memory_09_A": "Explain in a comment why pinned memory transfers are faster. (Hint: DMA, no intermediate staging buffer).",
"Fundamentals_Memory_09_B": "Remember to use `cudaFreeHost` to free memory allocated with `cudaMallocHost`.",
"Fundamentals_Memory_09_C": "Time both HtoD and DtoH transfers with pinned memory and compare to the pageable case.",
"Fundamentals_Memory_09_D": "What is the downside of using pinned memory? (It's a limited resource and reduces memory available to the OS and other applications).",
"Fundamentals_Memory_09_E": "Allocate a very large amount of pinned memory (e.g., more than half your system RAM). Does it succeed? What happens to your system's responsiveness?",
"Fundamentals_Memory_09_F": "Write a function that takes a boolean `use_pinned_memory` and performs the timed transfer, abstracting the allocation/deallocation logic.",
"Fundamentals_Memory_09_G": "Is pinned memory required for `cudaMemcpyAsync` to be truly asynchronous and overlap with kernel execution? (Yes). Explain why.",
"Fundamentals_Memory_09_H": "Try allocating pinned memory with the `cudaHostAllocMapped` flag.",
"Fundamentals_Memory_09_I": "Try passing a pointer from `cudaMallocHost` to `free()` instead of `cudaFreeHost`. What happens?",
"Fundamentals_Memory_09_J": "Create a table in your output showing Pageable HtoD, Pinned HtoD, Pageable DtoH, and Pinned DtoH bandwidths.",
"Fundamentals_Memory_10": "Allocate a 2D array on the host (e.g., `M` rows, `N` cols) and represent it as a 1D array in row-major order. Allocate a corresponding 2D array on the device using `cudaMallocPitch`. Copy the host array to the device using `cudaMemcpy2D`. Explain in comments the purpose of the 'pitch' value.",
"Fundamentals_Memory_10_A": "Write a kernel that accesses the 2D array on the device. To access element (x, y), a thread must use the pitch: `T* row = (T*)((char*)d_ptr + y * pitch); T element = row[x];`.",
"Fundamentals_Memory_10_B": "After modifying the 2D array on the device with a kernel, copy it back to the host using `cudaMemcpy2D` and verify the results.",
"Fundamentals_Memory_10_C": "Explain why the pitch returned by `cudaMallocPitch` may be larger than `width * sizeof(element)`. (Hint: alignment for coalescing).",
"Fundamentals_Memory_10_D": "Perform a 2D device-to-device copy using `cudaMemcpy2D`.",
"Fundamentals_Memory_10_E": "Use `cudaMemset2D` to initialize a pitched 2D allocation to zero.",
"Fundamentals_Memory_10_F": "What happens if you use `cudaMalloc` and `cudaMemcpy` for a 2D array instead of the `Pitch`/`2D` versions? It works, but why might it be suboptimal for performance?",
"Fundamentals_G_Memory_10_G": "Free the pitched memory using `cudaFree`.",
"Fundamentals_Memory_10_H": "Write a program to allocate a 3D array using `cudaMalloc3D` and copy data to it using `cudaMemcpy3D`.",
"Fundamentals_Memory_10_I": "Demonstrate copying a sub-matrix from host to device using `cudaMemcpy2D`.",
"Fundamentals_Memory_10_J": "Create a C++ class that encapsulates a 2D pitched device allocation, storing the pointer and pitch, and providing a safe access method.",
"Fundamentals_Memory_11": "Define a `__constant__` memory variable in your CUDA file (e.g., `__constant__ int my_const;`). In the host code, use `cudaMemcpyToSymbol` to copy a value (e.g., 42) into this variable. Write a kernel that reads from `my_const` and writes its value into an output array.",
"Fundamentals_Memory_11_A": "Explain in a comment the properties of constant memory (cached, read-only for kernel, low latency when all threads in a warp access the same address).",
"Fundamentals_Memory_11_B": "Use a struct as the type for your `__constant__` variable. Copy a host struct to it and access its members in the kernel.",
"Fundamentals_Memory_11_C": "Use an array in constant memory, e.g., `__constant__ float coefficients[16];`. Copy 16 floats from the host and use them in a kernel.",
"Fundamentals_Memory_11_D": "What is the size limit for constant memory? (Typically 64KB).",
"Fundamentals_Memory_11_E": "Demonstrate the performance benefit: write two kernels, one where each thread reads a configuration parameter from global memory, and one where it reads from constant memory. Time both.",
"Fundamentals_Memory_11_F": "Can you write to a `__constant__` variable from within a kernel? Try it and observe the compiler error.",
"Fundamentals_Memory_11_G": "Use `cudaGetSymbolAddress` to get the device address of the constant variable.",
"Fundamentals_Memory_11_H": "Use the alternative syntax for `cudaMemcpyToSymbol` which takes the symbol as a variable: `cudaMemcpyToSymbol(my_const, ...)`.",
"Fundamentals_Memory_11_I": "Try to use `cudaMemcpyFromSymbol` to read the value of the constant variable back to the host.",
"Fundamentals_Memory_11_J": "What happens if you forget to copy a value to the constant memory before the kernel uses it? What is its initial value?",
"Fundamentals_Memory_12": "Write a C++ wrapper class for device memory. The constructor should accept a size, call `cudaMalloc`, and store the pointer. The destructor should call `cudaFree`. This is a basic implementation of the RAII (Resource Acquisition Is Initialization) principle for CUDA pointers.",
"Fundamentals_Memory_12_A": "Add a copy constructor and a copy assignment operator that perform a deep copy (allocating new device memory and doing a `cudaMemcpyDeviceToDevice`).",
"Fundamentals_Memory_12_B": "Add a move constructor and a move assignment operator to allow for efficient transfers of ownership of the device pointer.",
"Fundamentals_Memory_12_C": "Add a member function `get()` that returns the raw device pointer.",
"Fundamentals_Memory_12_D": "Add member functions `copy_to_device(const T* host_ptr, size_t count)` and `copy_to_host(T* host_ptr, size_t count)`.",
"Fundamentals_Memory_12_E": "Make the class a template `template <typename T>` so it can manage any data type.",
"Fundamentals_Memory_12_F": "Instantiate your class in a scope (e.g., inside a function or an `if` block) and verify that the destructor is called automatically when the object goes out of scope.",
"Fundamentals_Memory_12_G": "Add error checking to all CUDA calls within the class methods.",
"Fundamentals_Memory_12_H": "Overload the `operator[]` (note: this is advanced as it can't directly access memory, it would need to perform a single-element copy, making it very slow but a good C++ exercise).",
"Fundamentals_Memory_12_I": "Create a similar RAII wrapper class for CUDA events (`cudaEvent_t`).",
"Fundamentals_Memory_12_J": "Create a similar RAII wrapper for pinned host memory (`cudaMallocHost`/`cudaFreeHost`).",
"Fundamentals_Memory_13": "Demonstrate a `cudaErrorMemoryAllocation` error by attempting to allocate an impossibly large amount of memory on the device (e.g., 100 GB). Catch and report the error gracefully.",
"Fundamentals_Memory_13_A": "Use `cudaMemGetInfo` to find out how much free memory is available, then try to allocate slightly more than that amount to trigger the error.",
"Fundamentals_Memory_13_B": "Write a loop that keeps allocating 1GB chunks of memory until `cudaMalloc` fails. Print how many GB were successfully allocated before failure.",
"Fundamentals_Memory_13_C": "Does a failed `cudaMalloc` return a `nullptr` and a success code, or does it return an error code directly? Check the documentation and your test.",
"Fundamentals_Memory_13_D": "Write a `try_alloc(size_t bytes)` function that returns `true` if `cudaMalloc` succeeds (and then immediately frees it) and `false` otherwise.",
"Fundamentals_Memory_13_E": "Check for the `cudaErrorMemoryAllocation` error specifically by comparing the return value to the enum.",
"Fundamentals_Memory_13_F": "Can `cudaMallocHost` (pinned memory allocation) also fail with a memory allocation error? Try it.",
"Fundamentals_t_Memory_13_G": "What happens if you pass a negative size to `cudaMalloc`?",
"Fundamentals_Memory_13_H": "Explain in a comment that even if a large allocation succeeds, it might make the GPU unusable for display or other applications.",
"Fundamentals_Memory_13_I": "Call `cudaGetLastError` after a failed malloc to retrieve the error.",
"Fundamentals_Memory_13_J": "If a `cudaMalloc` fails, is the pointer it was supposed to write to modified? Or does it retain its previous value?",
"Fundamentals_Memory_14": "Allocate a device array. Pass its pointer to a kernel. Inside the kernel, deliberately write to an out-of-bounds index (e.g., `d_ptr[BIG_NUMBER]`). Run the program using `cuda-memcheck` and analyze its report to find the memory error.",
"Fundamentals_Memory_14_A": "Perform an out-of-bounds read instead of a write and analyze the `cuda-memcheck` report.",
"Fundamentals_Memory_14_B": "In a kernel launched with N threads for an array of size N, write to `d_ptr[threadIdx.x + 1]`. Thread N-1 will go out of bounds. Run with `cuda-memcheck`.",
"Fundamentals_Memory_14_C": "Write to a negative index, like `d_ptr[-1]`, and check the report.",
"Fundamentals_Memory_14_D": "The error reported by `cuda-memcheck` is an asynchronous error. Show that without `cuda-memcheck`, the program may run and exit without any visible error, potentially producing wrong results.",
"Fundamentals_Memory_14_E": "Use `cudaDeviceSynchronize` and proper error checking. An out-of-bounds access often results in `cudaErrorIllegalAddress`. Catch and report this error.",
"Fundamentals_Memory_14_F": "Run `cuda-memcheck` with the `--tool racecheck` option to look for race conditions.",
"Fundamentals_Memory_14_G": "Run `cuda-memcheck` with the `--tool synccheck` option to look for synchronization errors.",
"Fundamentals_Memory_14_H": "Allocate two separate device arrays. Try to use an index that is valid for the second array on a pointer to the first array.",
"Fundamentals_Memory_14_I": "Pass a `nullptr` to the kernel and have the kernel try to read or write from it. Analyze the `cuda-memcheck` report for this null pointer dereference.",
"Fundamentals_Memory_14_J": "Explain what a 'memory fence' is and how it relates to debugging memory errors.",
"Fundamentals_Memory_15": "Declare a global device variable using `__device__`. Write a kernel that modifies this variable. Write another kernel that reads it. Demonstrate how its value persists across kernel launches within the same application.",
"Fundamentals_Memory_15_A": "Initialize the `__device__` variable at declaration, e.g., `__device__ int my_global_counter = 0;`.",
"Fundamentals_Memory_15_B": "Write a kernel `increment_counter()` that has each thread perform `atomicAdd(&my_global_counter, 1);`. Launch it, then launch a second kernel `read_counter(int* out)` that has thread 0 write the value of `my_global_counter` to an output variable for verification.",
"Fundamentals_Memory_15_C": "What is the scope and lifetime of a `__device__` variable? Explain in a comment. (Scope of the entire CUDA context, lifetime of the application).",
"Fundamentals_Memory_15_D": "Compare `__device__` memory to `__constant__` memory. What are the key differences? (Writable by kernel, not cached in the same way).",
"Fundamentals_Memory_15_E": "Use `cudaMemcpyToSymbol` and `cudaMemcpyFromSymbol` to initialize and read the `__device__` variable from the host.",
"Fundamentals_Memory_15_F": "Declare an array using `__device__`, e.g., `__device__ float lookup_table[256];`. Initialize it from the host and use it in a kernel.",
"Fundamentals_Memory_15_G": "Is `__device__` memory typically fast or slow to access compared to global memory from `cudaMalloc`? (It is global memory, so the performance is the same).",
"Fundamentals_Memory_15_H": "Declare a `__device__` variable in a header file. Does this cause problems? (Yes, can lead to multiple definitions unless handled with `extern`).",
"Fundamentals_Memory_15_I": "What happens to the value of a `__device__` variable when the application exits?",
"Fundamentals_Memory_15_J": "Use `cudaGetSymbolAddress` to get a device pointer to the `__device__` variable, which you can then pass to other API functions like `cudaMemset`.",
"Fundamentals_VectorAdd_01": "Implement the canonical first parallel program: element-wise vector addition. Create two input vectors (A and B) and one output vector (C) of size 256. Initialize A and B on the host. Launch a kernel with one block of 256 threads. Each thread `i` should compute `C[i] = A[i] + B[i]`.",
"Fundamentals_VectorAdd_01_A": "Use `float`s for the vectors instead of `int`s.",
"Fundamentals_VectorAdd_01_B": "Initialize vector A with the values `i` and vector B with `100-i`. Verify that `C[i]` is always 100.",
"Fundamentals_VectorAdd_01_C": "Write a separate `__global__` kernel to initialize the input vectors A and B on the GPU, instead of initializing on the host and copying.",
"Fundamentals_VectorAdd_01_D": "Perform the vector addition in-place: `A[i] = A[i] + B[i]`.",
"Fundamentals_VectorAdd_01_E": "Implement vector subtraction: `C[i] = A[i] - B[i]`.",
"Fundamentals_VectorAdd_01_F": "Implement element-wise vector multiplication: `C[i] = A[i] * B[i]`.",
"Fundamentals_VectorAdd_01_G": "Implement vector-scalar addition: `B[i] = A[i] + 5;` where 5 is a scalar value passed to the kernel.",
"Fundamentals_VectorAdd_01_H": "Use `cudaEvent`s to time only the kernel execution time.",
"Fundamentals_VectorAdd_01_I": "Refactor the host code into functions: `init_vectors()`, `run_kernel()`, `verify_results()`.",
"Fundamentals_VectorAdd_01_J": "Add error checking to all CUDA calls and the kernel launch.",
"Fundamentals_VectorAdd_02": "Generalize the vector addition program to handle vectors of a large size, e.g., 1,048,576 elements. This will require launching multiple blocks. Each thread must calculate its unique global index using `blockIdx.x * blockDim.x + threadIdx.x` to work on the correct element.",
"Fundamentals_VectorAdd_02_A": "Choose a block size of 256 threads. Calculate the required grid size for a vector of 1,048,576 elements.",
"Fundamentals_VectorAdd_02_B": "Make the vector size `N` and the block size `BLOCK_SIZE` configurable (e.g., using `const int` or `#define`). Calculate the grid size dynamically.",
"Fundamentals_VectorAdd_02_C": "Wrap the grid/block calculation and kernel launch into a helper function.",
"Fundamentals_VectorAdd_02_D": "Test your generalized kernel with several large `N` values (e.g., 10000, 500000, 2000000).",
"Fundamentals_VectorAdd_02_E": "Inside the kernel, add a `printf` from a single thread (e.g., `if (global_id == 12345)`) to inspect a value.",
"Fundamentals_VectorAdd_02_F": "Compare the execution time for `N=1M` vs `N=2M`. Is it roughly double?",
"Fundamentals_VectorAdd_02_G": "Implement vector division: `C[i] = A[i] / B[i]`. Add a check in the kernel to prevent division by zero.",
"Fundamentals_VectorAdd_02_H": "Pass `N` as an argument to the kernel.",
"Fundamentals_VectorAdd_02_I": "Create a `main` function that accepts `N` as a command-line argument.",
"Fundamentals_VectorAdd_02_J": "Run the program with `nvprof` or `nsight compute` to profile its performance.",
"Fundamentals_VectorAdd_03": "Modify your large vector addition program to handle input sizes that are not an exact multiple of the block size (e.g., 1,000,000 elements). Add a boundary check inside the kernel (`if (index < N) { ... }`) to prevent threads from accessing memory out of bounds.",
"Fundamentals_VectorAdd_03_A": "Test your program with `N = 1` and a block size of 256. Does it work correctly?",
"Fundamentals_VectorAdd_03_B": "Test with `N = 255`, `N = 256`, and `N = 257` with a block size of 256. Verify correctness for all cases.",
"Fundamentals_VectorAdd_03_C": "Explain in a comment why the grid calculation `(N + BLOCK_SIZE - 1) / BLOCK_SIZE` launches enough blocks to cover all elements.",
"Fundamentals_VectorAdd_03_D": "Remove the `if (index < N)` check and run the program for a non-multiple size with `cuda-memcheck`. Observe the out-of-bounds access errors.",
"Fundamentals_VectorAdd_03_E": "Is the boundary check computationally expensive? Why or why not? (It's a very cheap branch, and for full warps, there is often no divergence).",
"Fundamentals_VectorAdd_03_F": "Apply the same boundary check logic to a vector squaring kernel: `if (i < N) out[i] = in[i] * in[i]`.",
"Fundamentals_VectorAdd_03_G": "Apply the boundary check to a kernel that takes two input vectors and one output vector.",
"Fundamentals_VectorAdd_03_H": "Write a `main` function that tests a range of `N` values, some multiples of the block size and some not, and verifies all of them.",
"Fundamentals_VectorAdd_03_I": "Move the global index calculation and boundary check into a `__device__` helper function.",
"Fundamentals_VectorAdd_03_J": "Instead of an `if`, can you use arithmetic to prevent out-of-bounds access (e.g. `C[index] = ...` where index is somehow clamped)? Is this better or worse than a branch?",
"Fundamentals_VectorAdd_04": "Implement element-wise vector subtraction (`C[i] = A[i] - B[i]`).",
"Fundamentals_VectorAdd_04_A": "Implement `C[i] = B[i] - A[i]`.",
"Fundamentals_VectorAdd_04_B": "Implement for `double` data type.",
"Fundamentals_VectorAdd_04_C": "Implement `C[i] = A[i] - scalar`.",
"Fundamentals_VectorAdd_04_D": "Implement `C[i] = abs(A[i] - B[i])`.",
"Fundamentals_VectorAdd_04_E": "Implement in-place subtraction: `A[i] -= B[i]`.",
"Fundamentals_VectorAdd_04_F": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_04_G": "Implement for character arrays: `C[i] = A[i] - B[i]`.",
"Fundamentals_VectorAdd_04_H": "Implement using `__half` precision numbers if your GPU supports it.",
"Fundamentals_VectorAdd_04_I": "Time the subtraction kernel. Is it significantly different from addition?",
"Fundamentals_VectorAdd_04_J": "Verify the results of subtraction on the CPU.",
"Fundamentals_VectorAdd_05": "Implement element-wise vector multiplication, also known as the Hadamard product (`C[i] = A[i] * B[i]`).",
"Fundamentals_VectorAdd_05_A": "Implement `C[i] = A[i] * A[i]` (squaring).",
"Fundamentals_VectorAdd_05_B": "Implement for `double` data type.",
"Fundamentals_VectorAdd_05_C": "Implement `C[i] = A[i] * scalar`.",
"Fundamentals_VectorAdd_05_D": "Implement `C[i] = A[i] * B[i] * D[i]` (three-vector multiplication).",
"Fundamentals_VectorAdd_05_E": "Implement in-place multiplication: `A[i] *= B[i]`.",
"Fundamentals_VectorAdd_05_F": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_05_G": "Implement for `float` data.",
"Fundamentals_VectorAdd_05_H": "Implement `C[i] = pow(A[i], 2.5f)`.",
"Fundamentals_VectorAdd_05_I": "Time the multiplication kernel.",
"Fundamentals_VectorAdd_05_J": "Verify the results of multiplication on the CPU.",
"Fundamentals_VectorAdd_06": "Implement a kernel for the SAXPY (Single-precision A*X Plus Y) operation. The operation is `Y = a*X + Y`, where `a` is a scalar float, and `X` and `Y` are vectors of floats. The scalar `a` should be passed as an argument to the kernel.",
"Fundamentals_VectorAdd_06_A": "Implement the operation out-of-place: `Z[i] = a * X[i] + Y[i]`.",
"Fundamentals_VectorAdd_06_B": "The original SAXPY is in-place, modifying `Y`. Implement this version: `Y[i] = a * X[i] + Y[i]`.",
"Fundamentals_VectorAdd_06_C": "Implement DAXPY, the double-precision version of the operation.",
"Fundamentals_VectorAdd_06_D": "Time your SAXPY implementation for a large vector.",
"Fundamentals_VectorAdd_06_E": "Explain what 'fused multiply-add' (FMA) is and why an operation like SAXPY is well-suited for it.",
"Fundamentals_VectorAdd_06_F": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_06_G": "Verify the results of SAXPY on the CPU.",
"Fundamentals_VectorAdd_06_H": "Write a kernel for a generalized AXPY: `Y[i] = a*X[i] + b*Y[i] + c`, where a, b, c are scalars.",
"Fundamentals_VectorAdd_06_I": "Compare the performance of your SAXPY to the `cublasSaxpy` function from the cuBLAS library.",
"Fundamentals_VectorAdd_06_J": "Pass the scalar `a` via constant memory instead of as a kernel argument.",
"Fundamentals_VectorAdd_07": "Implement a kernel that squares every element in a vector: `output[i] = input[i] * input[i]`.",
"Fundamentals_VectorAdd_07_A": "Implement a kernel that cubes every element: `output[i] = input[i] * input[i] * input[i]`.",
"Fundamentals_VectorAdd_07_B": "Implement a kernel that computes the square root of every element: `output[i] = sqrtf(input[i])`.",
"Fundamentals_VectorAdd_07_C": "Implement an in-place version: `data[i] = data[i] * data[i]`.",
"Fundamentals_VectorAdd_07_D": "Handle `double` precision data.",
"Fundamentals_VectorAdd_07_E": "Implement a kernel that computes `x^n` where `n` is passed as a parameter: `output[i] = powf(input[i], n)`.",
"Fundamentals_VectorAdd_07_F": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_07_G": "Verify the results on the CPU.",
"Fundamentals_VectorAdd_07_H": "Implement `output[i] = 1.0f / input[i]` (reciprocal).",
"Fundamentals_VectorAdd_07_I": "Implement `output[i] = sinf(input[i])`.",
"Fundamentals_VectorAdd_07_J": "Time the squaring kernel.",
"Fundamentals_VectorAdd_08": "Implement a kernel that adds a scalar value to every element of a vector: `output[i] = input[i] + scalar_value`.",
"Fundamentals_VectorAdd_08_A": "Implement an in-place version: `data[i] += scalar_value`.",
"Fundamentals_VectorAdd_08_B": "Implement scalar multiplication: `output[i] = input[i] * scalar_value`.",
"Fundamentals_VectorAdd_08_C": "Implement scalar division: `output[i] = input[i] / scalar_value`.",
"Fundamentals_VectorAdd_08_D": "Implement scalar subtraction: `output[i] = input[i] - scalar_value`.",
"Fundamentals_VectorAdd_08_E": "Implement `output[i] = scalar_value - input[i]`.",
"Fundamentals_VectorAdd_08_F": "Handle `double` precision data.",
"Fundamentals_VectorAdd_08_G": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_08_H": "Verify the results on the CPU.",
"Fundamentals_VectorAdd_08_I": "Pass the scalar via constant memory.",
"Fundamentals_VectorAdd_08_J": "Implement `output[i] = max(input[i], scalar_value)`.",
"Fundamentals_VectorAdd_09": "Write a kernel that computes `D[i] = (A[i] + B[i]) * scalar`. This demonstrates passing multiple vectors and a scalar.",
"Fundamentals_VectorAdd_09_A": "Implement `D[i] = A[i] * B[i] + C[i]` where C is a third input vector.",
"Fundamentals_VectorAdd_09_B": "Implement linear interpolation (lerp): `C[i] = A[i] * (1.0 - t) + B[i] * t`, where `t` is a scalar float.",
"Fundamentals_VectorAdd_09_C": "Implement `D[i] = (A[i] > B[i]) ? A[i] * s1 : B[i] * s2`, where `s1` and `s2` are scalars.",
"Fundamentals_VectorAdd_09_D": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_09_E": "Implement with `double` precision.",
"Fundamentals_VectorAdd_09_F": "Verify the results on the CPU.",
"Fundamentals_VectorAdd_09_G": "Implement `D[i] = (A[i] * s1) + (B[i] * s2)`.",
"Fundamentals_VectorAdd_09_H": "Pass all scalars and vector pointers to the kernel.",
"Fundamentals_VectorAdd_09_I": "Calculate the dot product of two vectors (this is a reduction, a more advanced topic, but can be done naively with atomics).",
"Fundamentals_VectorAdd_09_J": "Implement `D[i] = sqrt(A[i]*A[i] + B[i]*B[i])` (vector magnitude for 2D vectors).",
"Fundamentals_VectorAdd_10": "Compare the performance of your CUDA vector addition kernel against a simple, single-threaded C++ loop on the CPU. Time both versions for a very large vector (e.g., 10 million elements). Remember to only time the computation, not the data transfers, for a fair comparison.",
"Fundamentals_VectorAdd_10_A": "Include the HtoD and DtoH data transfer times in the GPU timing. Now which is faster for a moderately sized vector? This shows the impact of transfer overhead.",
"Fundamentals_VectorAdd_10_B": "Write a multi-threaded CPU version of vector addition (e.g., using OpenMP) and compare it to the GPU version.",
"Fundamentals_VectorAdd_10_C": "Plot a graph of performance (elements/sec) vs. vector size (N) for both CPU and GPU. Find the crossover point where the GPU becomes faster.",
"Fundamentals_VectorAdd_10_D": "For the CPU timing, use `std::chrono::high_resolution_clock`.",
"Fundamentals_VectorAdd_10_E": "For the GPU timing, use `cudaEvent`s around the kernel launch only.",
"Fundamentals_VectorAdd_10_F": "Make sure your CPU code is compiled with optimizations (e.g., `-O3` in GCC/Clang) for a fair comparison. Modern compilers can auto-vectorize the CPU loop using SSE/AVX instructions.",
"Fundamentals_VectorAdd_10_G": "Calculate and print the speedup factor (CPU time / GPU time).",
"Fundamentals_VectorAdd_10_H": "Perform the comparison for a more compute-intensive kernel, like `sin(cos(x))`. Does the GPU speedup increase?",
"Fundamentals_VectorAdd_10_I": "Perform the comparison on a machine with a high-end CPU and a low-end GPU, and vice-versa. Observe how the results change.",
"Fundamentals_VectorAdd_10_J": "Explain in a comment why GPUs excel at this type of 'embarrassingly parallel' problem.",
"Fundamentals_VectorAdd_11": "Implement vector addition using a 'grid-stride loop' inside the kernel. Instead of each thread processing only one element, launch a smaller grid of threads and have each thread process multiple elements in a strided loop: `for (int i = thread_id; i < N; i += grid_size) { ... }`. This makes the kernel independent of the number of threads launched.",
"Fundamentals_VectorAdd_11_A": "In the loop, `thread_id` is the global index `blockIdx.x * blockDim.x + threadIdx.x`. `grid_size` is the total number of threads launched `gridDim.x * blockDim.x`. Implement this.",
"Fundamentals_VectorAdd_11_B": "Launch your grid-stride kernel for a vector of size 1M, but only launch a grid of 1024 threads total. Verify the correctness.",
"Fundamentals_VectorAdd_11_C": "Explain the advantages of a grid-stride loop. (Kernel is independent of launch size, can handle any `N`, better hardware utilization if launch is small).",
"Fundamentals_VectorAdd_11_D": "Is there a performance difference between a standard launch (one thread per element) and a grid-stride launch (many elements per thread) for a problem that fits in one launch? Time both.",
"Fundamentals_VectorAdd_11_E": "Implement a vector squaring kernel using a grid-stride loop.",
"Fundamentals_VectorAdd_11_F": "Can you still get out-of-bounds errors with a grid-stride loop if `N` is not a multiple of anything? (No, the loop condition `i < N` handles it perfectly).",
"Fundamentals_VectorAdd_11_G": "Launch your grid-stride kernel with a number of blocks equal to the number of SMs on your GPU, and a reasonable block size (e.g. 128 or 256). This is a common performance heuristic.",
"Fundamentals_VectorAdd_11_H": "Modify the grid-stride loop to process elements backwards from `N-1` down to 0.",
"Fundamentals_VectorAdd_11_I": "Use a grid-stride loop to initialize a very large array to a specific value.",
"Fundamentals_VectorAdd_11_J": "If you launch more threads than elements `N`, the grid-stride loop will have some threads do no work. Verify this is handled correctly.",
"Fundamentals_VectorAdd_12": "Implement element-wise addition for two 2D matrices. Use a 2D grid of 2D blocks. Each thread should calculate its 2D global index `(x, y)` and use it to access the matrix elements.",
"Fundamentals_VectorAdd_12_A": "Matrices are stored in 1D memory (row-major). The 1D index is `idx = global_y * width + global_x`. Implement the matrix add using this.",
"Fundamentals_VectorAdd_12_B": "Add a boundary check: `if (global_x < width && global_y < height)` to handle matrix dimensions that are not perfect multiples of the block dimensions.",
"Fundamentals_VectorAdd_12_C": "Implement matrix-scalar addition.",
"Fundamentals_VectorAdd_12_D": "Implement matrix subtraction and multiplication.",
"Fundamentals_VectorAdd_12_E": "Use `cudaMallocPitch` and `cudaMemcpy2D` for your matrices and modify the kernel access logic to use the pitch.",
"Fundamentals_VectorAdd_12_F": "Implement a matrix transpose: `output[x * height + y] = input[y * width + x]`. Note this can have memory coalescing issues.",
"Fundamentals_VectorAdd_12_G": "Launch with a 1D grid of 1D blocks instead. Each thread must manually compute its `(x,y)` coordinate from its global 1D ID: `int x = id % width; int y = id / width;`.",
"Fundamentals_VectorAdd_12_H": "Verify the matrix addition result on the CPU.",
"Fundamentals_VectorAdd_12_I": "Write a kernel to set a matrix to the identity matrix (1s on the diagonal, 0s elsewhere).",
"Fundamentals_VectorAdd_12_J": "Time the matrix addition kernel.",
"Fundamentals_VectorAdd_13": "Write a kernel that performs a conditional operation. For two input vectors A and B, compute `C[i] = (A[i] > B[i]) ? A[i] : B[i]` (element-wise maximum).",
"Fundamentals_VectorAdd_13_A": "Implement element-wise minimum.",
"Fundamentals_VectorAdd_13_B": "Implement `C[i] = (A[i] > threshold) ? 1 : 0` (thresholding).",
"Fundamentals_VectorAdd_13_C": "Implement `C[i] = (A[i] > 0) ? A[i] : 0` (ReLU activation function).",
"Fundamentals_VectorAdd_13_D": "Use `fmaxf()` from the CUDA math library instead of the ternary operator.",
"Fundamentals_VectorAdd_13_E": "Implement with `double` precision.",
"Fundamentals_VectorAdd_13_F": "Handle large vectors with boundary checks.",
"Fundamentals_VectorAdd_13_G": "Implement `C[i] = (A[i] == B[i]) ? 1 : 0`.",
"Fundamentals_VectorAdd_13_H": "Implement a clamp function: `C[i] = min(max(A[i], min_val), max_val)`.",
"Fundamentals_VectorAdd_13_I": "Verify the results on the CPU.",
"Fundamentals_VectorAdd_13_J": "Explain warp divergence. In the kernel `(A[i] > B[i]) ? X : Y`, if some threads in a warp take branch X and others take branch Y, what is the performance impact?",
"Fundamentals_VectorAdd_14": "Implement vector addition for `double` precision floating-point numbers instead of `float`s.",
"Fundamentals_VectorAdd_14_A": "Ensure all host and device allocations use `sizeof(double)`.",
"Fundamentals_VectorAdd_14_B": "Time the `double` precision kernel and compare it to the `float` version. On most consumer GPUs, double precision performance is significantly lower.",
"Fundamentals_VectorAdd_14_C": "Query the device properties for `double` precision performance relative to single precision.",
"Fundamentals_VectorAdd_14_D": "Implement DAXPY using `double`s.",
"Fundamentals_VectorAdd_14_E": "Verify the `double` precision results on the CPU. Note the potential for tiny floating point discrepancies.",
"Fundamentals_VectorAdd_14_F": "Implement vector addition for `short` integers.",
"Fundamentals_VectorAdd_14_G": "Implement vector addition for `long long int`.",
"Fundamentals_VectorAdd_14_H": "Check if your GPU supports native half-precision (`__half`) arithmetic and try to implement vector add with it.",
"Fundamentals_VectorAdd_14_I": "What happens if you mismatch types? E.g., device memory is `double*` but the kernel signature is `float*`?",
"Fundamentals_VectorAdd_14_J": "Handle large `double` vectors with boundary checks.",
"Fundamentals_VectorAdd_15": "Implement a 'fused' kernel that performs two operations at once. For example, `C[i] = A[i] + B[i]` and `D[i] = A[i] - B[i]`. This can improve performance by increasing arithmetic intensity.",
"Fundamentals_VectorAdd_15_A": "Implement a fused kernel that computes SAXPY and also the dot product of two vectors (using atomic adds for the reduction part).",
"Fundamentals_VectorAdd_15_B": "Compare the performance of the fused kernel (`C=A+B`, `D=A-B`) against two separate kernel launches (one for addition, one for subtraction). The fused kernel should be faster.",
"Fundamentals_VectorAdd_15_C": "Explain 'arithmetic intensity' (ratio of math operations to memory operations). How does fusing kernels improve it?",
"Fundamentals_VectorAdd_15_D": "Implement a kernel that finds the min and max of a vector in a single pass (requires a parallel reduction, which is more advanced).",
"Fundamentals_VectorAdd_15_E": "Fuse a multiply and an add: `C[i] = A[i] * scalar1; D[i] = B[i] + scalar2;`",
"Fundamentals_VectorAdd_15_F": "Write a kernel that computes `C[i] = A[i] + B[i]` and writes the result to `C`, and also writes `A[i]` to another buffer `D` if `A[i]` is greater than some threshold.",
"Fundamentals_VectorAdd_15_G": "Fuse a copy and a scale operation: `B[i] = A[i] * scalar`.",
"Fundamentals_VectorAdd_15_H": "Handle large vectors with boundary checks in your fused kernel.",
"Fundamentals_VectorAdd_15_I": "The `sinf` and `cosf` functions can sometimes be computed together more cheaply. Write a kernel that computes `C[i] = sinf(A[i])` and `D[i] = cosf(A[i])` and compare to separate kernels.",
"Fundamentals_VectorAdd_15_J": "A kernel that computes `C[i] = a*A[i] + b*B[i]` is a fused operation.",
"Fundamentals_ErrorCheck_01": "Create a C/C++ preprocessor macro `CHECK(call)` that wraps a CUDA API call. The macro should check the returned `cudaError_t` value. If it's not `cudaSuccess`, it should print the error code, the error string from `cudaGetErrorString`, the file name (`__FILE__`), and the line number (`__LINE__`), and then exit the program.",
"Fundamentals_ErrorCheck_01_A": "Make the macro a `do { ... } while(0)` block to ensure it behaves like a single statement.",
"Fundamentals_ErrorCheck_01_B": "Use `fprintf(stderr, ...)` to print the error message to standard error instead of standard out.",
"Fundamentals_ErrorCheck_01_C": "Instead of `exit(-1)`, use `cudaDeviceReset()` before exiting to clean up the context.",
"Fundamentals_ErrorCheck_01_D": "Create a second version of the macro, `CHECK_NO_EXIT`, that prints the error but does not terminate the program.",
"Fundamentals_ErrorCheck_01_E": "Use your macro on a call that is guaranteed to succeed, like `cudaGetDeviceCount`, to ensure it doesn't trigger incorrectly.",
"Fundamentals_ErrorCheck_01_F": "Use your macro on a call that is guaranteed to fail, like `cudaMalloc` for 100TB, to test its output.",
"Fundamentals_ErrorCheck_01_G": "Change the macro to also print the original CUDA call as a string using the `#call` preprocessor directive.",
"Fundamentals_ErrorCheck_01_H": "In C++, change the macro to throw a custom exception instead of exiting.",
"Fundamentals_ErrorCheck_01_I": "Write the error checking logic in a helper function `void check_error(cudaError_t code, const char* file, int line)` and have the macro call this function.",
"Fundamentals_ErrorCheck_01_J": "Add a conditional compilation flag (e.g., `_DEBUG`) so the `CHECK` macro only performs the check in debug builds, and is empty in release builds for performance.",
"Fundamentals_ErrorCheck_02": "Refactor your vector addition program from the previous section to use the `CHECK()` macro for every single CUDA API call (`cudaMalloc`, `cudaMemcpy`, `cudaFree`, etc.).",
"Fundamentals_ErrorCheck_02_A": "Wrap the `cudaMalloc` calls.",
"Fundamentals_ErrorCheck_02_B": "Wrap all three `cudaMemcpy` calls (HtoD for A, HtoD for B, DtoH for C).",
"Fundamentals_ErrorCheck_02_C": "Wrap all `cudaFree` calls.",
"Fundamentals_ErrorCheck_02_D": "If you are using CUDA events, wrap `cudaEventCreate`, `cudaEventRecord`, and `cudaEventDestroy`.",
"Fundamentals_ErrorCheck_02_E": "Deliberately introduce an error (e.g., pass a null pointer to `cudaMemcpy`) and see your macro in action.",
"Fundamentals_ErrorCheck_02_F": "Place all CUDA calls in your program on a separate line so the `CHECK` macro can wrap them cleanly.",
"Fundamentals_ErrorCheck_02_G": "Does `cudaDeviceSynchronize` need to be wrapped? Yes, it can return asynchronous errors.",
"Fundamentals_ErrorCheck_02_H": "Does `cudaDeviceReset` need to be wrapped? Yes.",
"Fundamentals_ErrorCheck_02_I": "Go back to the device query programs from `Fundamentals_Setup` and add `CHECK` macros to all API calls.",
"Fundamentals_ErrorCheck_02_J": "Add the `CHECK` macro around a call to `cudaGetLastError()`.",
"Fundamentals_ErrorCheck_03": "Kernel launches do not return an error code directly. Add error checking for a kernel launch by calling `cudaPeekAtLastError()` immediately after the launch and `cudaDeviceSynchronize()` sometime later. Wrap both of these calls with your `CHECK()` macro.",
"Fundamentals_ErrorCheck_03_A": "Explain the difference between `cudaPeekAtLastError` and `cudaGetLastError` in a comment. (Peek does not reset the error state).",
"Fundamentals_ErrorCheck_03_B": "Create a `CHECK_KERNEL()` macro that specifically wraps these two calls for convenience.",
"Fundamentals_ErrorCheck_03_C": "Demonstrate its use by launching a kernel with an invalid configuration (e.g., too many threads) and catching the error with `CHECK(cudaPeekAtLastError())`.",
"Fundamentals_ErrorCheck_03_D": "Demonstrate catching an asynchronous error from inside the kernel (e.g. an illegal memory access) using `CHECK(cudaDeviceSynchronize())`.",
"Fundamentals_ErrorCheck_03_E": "Why is it good practice to check for errors both immediately after launch and at synchronization? (Catch config errors early, catch execution errors later).",
"Fundamentals_ErrorCheck_03_F": "What happens if you have two bad kernel launches in a row, but only check for an error after the second one? The error from the first launch may be reported.",
"Fundamentals_ErrorCheck_03_G": "Is `cudaGetLastError()` a good way to check for kernel launch errors? (Yes, it's often used for this).",
"Fundamentals_ErrorCheck_03_H": "In a program with multiple streams, `cudaDeviceSynchronize` is a heavyweight operation. What is a more fine-grained alternative? (`cudaStreamSynchronize`).",
"Fundamentals_ErrorCheck_03_I": "Place a `CHECK(cudaGetLastError())` before your kernel launch to clear any prior unrelated errors.",
"Fundamentals_ErrorCheck_03_J": "Refactor your vector add program to include post-launch error checking.",
"Fundamentals_ErrorCheck_04": "Write a program that deliberately triggers a `cudaErrorInvalidValue` by passing a `nullptr` as the destination for `cudaMemcpy`. Use your `CHECK()` macro to catch and report the error cleanly.",
"Fundamentals_ErrorCheck_04_A": "Trigger `cudaErrorInvalidValue` by passing a negative size to `cudaMalloc`.",
"Fundamentals_ErrorCheck_04_B": "Trigger `cudaErrorInvalidValue` by passing an invalid `kind` to `cudaMemcpy`, e.g., `(cudaMemcpyKind)123`.",
"Fundamentals_ErrorCheck_04_C": "Trigger `cudaErrorInvalidValue` by calling `cudaSetDevice` with a negative device number.",
"Fundamentals_ErrorCheck_04_D": "Trigger the error by passing a `nullptr` for the pointer-to-a-pointer in `cudaMalloc` (`cudaMalloc(nullptr, 100)`)",
"Fundamentals_ErrorCheck_04_E": "Trigger the error by passing a `nullptr` for the event in `cudaEventCreate`.",
"Fundamentals_ErrorCheck_04_F": "Look up the documentation for `cudaErrorInvalidValue` and find another API call that can trigger it.",
"Fundamentals_ErrorCheck_04_G": "Does this error occur synchronously or asynchronously?",
"Fundamentals_ErrorCheck_04_H": "Create a test suite function that deliberately triggers and checks for several different invalid value errors.",
"Fundamentals_ErrorCheck_04_I": "Pass a valid device pointer but a `nullptr` host pointer to `cudaMemcpy` HtoD. What error does this cause?",
"Fundamentals_ErrorCheck_04_J": "Pass a size of 0 to `cudaMemcpy`. Is this an error?",
"Fundamentals_ErrorCheck_05": "Write a program that deliberately triggers a `cudaErrorInvalidDevice` by trying to `cudaSetDevice()` to a device index that doesn't exist (e.g., 99). Use your `CHECK()` macro to catch it.",
"Fundamentals_ErrorCheck_05_A": "First, get the device count. Then, try to set the device to `count` (which is an invalid index, since they are 0-based).",
"Fundamentals_ErrorCheck_05_B": "Look up the documentation for `cudaErrorInvalidDevice`. What other functions can return this error?",
"Fundamentals_ErrorCheck_05_C": "Is this a synchronous or asynchronous error?",
"Fundamentals_ErrorCheck_05_D": "Write a program that checks `cudaDeviceCanAccessPeer` with an invalid peer device index.",
"Fundamentals_ErrorCheck_05_E": "Try to get device properties for an invalid device index with `cudaGetDeviceProperties`.",
"Fundamentals_ErrorCheck_05_F": "What happens if you have 0 CUDA devices and you try to `cudaSetDevice(0)`?",
"Fundamentals_ErrorCheck_05_G": "Can a kernel launch fail with `cudaErrorInvalidDevice`? (Unlikely, as the context is already established).",
"Fundamentals_ErrorCheck_05_H": "Write a robust function `bool set_device(int id)` that checks the device count first and only calls `cudaSetDevice` if the ID is valid, returning `false` otherwise.",
"Fundamentals_ErrorCheck_05_I": "Trigger `cudaErrorNoDevice` by trying to run a CUDA program on a system with no NVIDIA driver or no CUDA-capable GPU.",
"Fundamentals_ErrorCheck_05_J": "Trigger `cudaErrorDeviceAlreadyInUse` (this is harder, might require multiple processes).",
"Fundamentals_ErrorCheck_06": "Create a C++ helper function `void checkKernelLaunch()` that calls `cudaDeviceSynchronize()` and checks for errors. Call this function after every kernel launch in your programs to ensure the kernel executed without an asynchronous error.",
"Fundamentals_ErrorCheck_06_A": "Modify the function to also call `cudaGetLastError()` first to catch any launch configuration errors.",
"Fundamentals_ErrorCheck_06_B": "Make the function take `const char* kernel_name`, `const char* file`, and `int line` as arguments so it can print a more informative error message.",
"Fundamentals_ErrorCheck_06_C": "Create a macro `CHECK_KERNEL(kernel_launch)` that expands to `{ kernel_launch; checkKernelLaunch(...); }`.",
"Fundamentals_ErrorCheck_06_D": "Demonstrate its use by checking a successful kernel launch.",
"Fundamentals_ErrorCheck_06_E": "Demonstrate its use by checking a kernel that fails due to an out-of-bounds access.",
"Fundamentals_ErrorCheck_06_F": "In a program with multiple, sequential kernel launches, place the check after each one to pinpoint which one is failing.",
"Fundamentals_ErrorCheck_06_G": "Explain why calling this check function only at the end of the program is not as useful for debugging.",
"Fundamentals_ErrorCheck_06_H": "Does `cudaDeviceSynchronize` have a performance cost? (Yes, it stalls the CPU). Explain when it's appropriate to use (debugging, final result needed) and when not (in a tight loop of asynchronous operations).",
"Fundamentals_ErrorCheck_06_I": "Write a version of the function for streams: `checkStream(cudaStream_t stream)` which calls `cudaStreamSynchronize`.",
"Fundamentals_ErrorCheck_06_J": "In your helper function, if an error is found, print the error string using `cudaGetErrorString`.",
"Fundamentals_ErrorCheck_07": "Explain the difference between synchronous and asynchronous CUDA errors. Write a code snippet that would cause a synchronous error (e.g., invalid `cudaMalloc` size) and another that would cause an asynchronous error (e.g., out-of-bounds access inside a kernel).",
"Fundamentals_ErrorCheck_07_A": "Provide a list of 3 common synchronous CUDA API calls.",
"Fundamentals_ErrorCheck_07_B": "Provide a list of 3 common asynchronous CUDA API calls (`cudaMemcpyAsync`, kernel launch, etc.).",
"Fundamentals_ErrorCheck_07_C": "How are synchronous errors reported? (Directly via the return code).",
"Fundamentals_ErrorCheck_07_D": "How are asynchronous errors reported? (Via a subsequent synchronous call or `cuda...Synchronize`).",
"Fundamentals_ErrorCheck_07_E": "Write code that triggers a synchronous error and use the `CHECK` macro to catch it immediately.",
"Fundamentals_ErrorCheck_07_F": "Write code that triggers an asynchronous error. Show that the API call itself returns `cudaSuccess` but `cudaDeviceSynchronize` later returns an error.",
"Fundamentals_ErrorCheck_07_G": "Is a kernel launch configuration error (e.g. too many threads) synchronous or asynchronous? (Asynchronous, but the error is usually available immediately via `cudaGetLastError`).",
"Fundamentals_ErrorCheck_07_H": "Explain the concept of the 'sticky' error state in CUDA.",
"Fundamentals_ErrorCheck_07_I": "Why does CUDA use an asynchronous model? (Performance, allows CPU and GPU to work in parallel).",
"Fundamentals_ErrorCheck_07_J": "Add detailed comments to your two code snippets explaining exactly why one error is synchronous and the other is asynchronous.",
"Fundamentals_ErrorCheck_08": "Run a program that has an out-of-bounds memory access inside the kernel (an asynchronous error) with and without proper post-launch error checking (`cudaDeviceSynchronize`). Observe that without it, the program may complete without any visible error, but potentially with incorrect results.",
"Fundamentals_ErrorCheck_08_A": "In the version without error checking, verify the output array and show that the results are wrong or nonsensical.",
"Fundamentals_ErrorCheck_08_B": "In the version with error checking, use your `CHECK` macro to catch and report the `cudaErrorIllegalAddress`.",
"Fundamentals_ErrorCheck_08_C": "Run the version without error checking under `cuda-memcheck`. Observe that `cuda-memcheck` finds the error even if the program itself doesn't report one.",
"Fundamentals_ErrorCheck_08_D": "Write a kernel that has a 1-in-a-million chance of accessing out of bounds. Run it in a loop. Without error checking, it might run correctly 99% of the time, highlighting the danger of silent errors.",
"Fundamentals_ErrorCheck_08_E": "Explain why this 'silent failure' is one of the most dangerous types of bugs in parallel programming.",
"Fundamentals_ErrorCheck_08_F": "Create another asynchronous error: an unspecified launch failure. E.g., a kernel that dereferences a null pointer.",
"Fundamentals_ErrorCheck_08_G": "Does `cudaPeekAtLastError` right after the launch catch an in-kernel memory error? (No, the kernel hasn't run yet).",
"Fundamentals_ErrorCheck_08_H": "Place the `CHECK(cudaDeviceSynchronize())` at the very end of `main`. Is this sufficient to know an error occurred? Is it good for debugging *where* it occurred?",
"Fundamentals_ErrorCheck_08_I": "Time your program with and without `cudaDeviceSynchronize` to see the performance overhead of synchronization.",
"Fundamentals_ErrorCheck_08_J": "Write a conclusion in comments: always use `cuda-memcheck` during development, and have robust error checking in your code.",
"Fundamentals_ErrorCheck_09": "Modify your `CHECK` macro to not exit, but instead throw a C++ exception. This allows for more sophisticated error handling in larger applications.",
"Fundamentals_ErrorCheck_09_A": "Define a custom exception class, e.g., `CudaException`, that inherits from `std::runtime_error` and stores the error code, error string, file, and line.",
"Fundamentals_ErrorCheck_09_B": "Wrap the `main` logic of your vector add program in a `try...catch` block to handle the `CudaException`.",
"Fundamentals_ErrorCheck_09_C": "In the `catch` block, print the detailed error message from the exception object.",
"Fundamentals_ErrorCheck_09_D": "Demonstrate that the exception is caught correctly by triggering an error (e.g., failed `cudaMalloc`).",
"Fundamentals_ErrorCheck_09_E": "Show how RAII (e.g., your device memory wrapper class) works well with exceptions to ensure `cudaFree` is called even if an error occurs mid-function.",
"Fundamentals_ErrorCheck_09_F": "What are the pros and cons of using exceptions vs. `exit()` for error handling in a CUDA application?",
"Fundamentals_ErrorCheck_09_G": "Can you throw an exception from `__device__` code? (No). Explain why.",
"Fundamentals_ErrorCheck_09_H": "Create a `CHECK_CUDA_THROW` macro for your new exception-based error handling.",
"Fundamentals_ErrorCheck_09_I": "Convert your `checkKernelLaunch` helper function to throw instead of exit.",
"Fundamentals_ErrorCheck_09_J": "Nest `try...catch` blocks to show how different levels of an application could handle CUDA errors differently.",
"Fundamentals_ErrorCheck_10": "Create a `GpuTimer` class in C++. The constructor should call `cudaEventCreate` for start and stop events. A `start()` method calls `cudaEventRecord`. A `stop()` method calls `cudaEventRecord` and `cudaEventSynchronize`. An `elapsed_ms()` method calls `cudaEventElapsedTime`. Add error checking to all CUDA calls.",
"Fundamentals_ErrorCheck_10_A": "The destructor of the `GpuTimer` class should call `cudaEventDestroy` for both events to prevent resource leaks.",
"Fundamentals_ErrorCheck_10_B": "Make the `stop()` method non-blocking by not calling `cudaEventSynchronize`. The user must call a separate `synchronize()` method before `elapsed_ms()` will be accurate.",
"Fundamentals_ErrorCheck_10_C": "Add a `stream` member and have the `start()` and `stop()` methods record events on that specific stream.",
"Fundamentals_ErrorCheck_10_D": "Use your `GpuTimer` class to time a vector addition kernel.",
"Fundamentals_ErrorCheck_10_E": "Use the timer to time a `cudaMemcpy` operation.",
"Fundamentals_ErrorCheck_10_F": "Create two timers to measure two different parts of your program sequentially.",
"Fundamentals_ErrorCheck_10_G": "What happens if you call `elapsed_ms()` before `stop()` has been called or before the event has completed? The result is undefined or an error.",
"Fundamentals_ErrorCheck_10_H": "Add a `reset()` method that allows the timer to be reused without creating a new object.",
"Fundamentals_ErrorCheck_10_I": "Make the class non-copyable by deleting the copy constructor and copy assignment operator, as `cudaEvent_t` handles cannot be trivially copied.",
"Fundamentals_ErrorCheck_10_J": "Use the timer in a loop to average the kernel execution time over many runs.",
"Intermediate_SharedMemory_01": "Write a naive matrix multiplication kernel (C = A * B). Each thread will be responsible for calculating a single element of the output matrix C. This will involve the thread reading one full row from A and one full column from B from global memory.",
"Intermediate_SharedMemory_01_A": "In the naive kernel, analyze the memory access pattern for matrix B. Explain why it is inefficient (uncoalesced/strided access).",
"Intermediate_SharedMemory_01_B": "Implement the naive kernel using a 2D grid of 2D blocks, where each thread's global (x, y) index corresponds to the C(x, y) element it computes.",
"Intermediate_SharedMemory_01_C": "Time the naive kernel for a 512x512 matrix multiplication.",
"Intermediate_SharedMemory_01_D": "Verify the correctness of your naive matrix multiplication against a simple CPU implementation.",
"Intermediate_SharedMemory_01_E": "Use `nsight compute` or `nvprof` to measure the global memory bandwidth and transaction count for the naive kernel. Note the high number of transactions.",
"Intermediate_SharedMemory_01_F": "Implement naive matrix-vector multiplication, where each thread block computes a portion of the output vector.",
"Intermediate_SharedMemory_02": "Implement tiled matrix multiplication. Each thread block will calculate one square sub-matrix (tile) of C. To do this, all threads in the block must cooperatively load the corresponding tiles of A and B into shared memory first.",
"Intermediate_SharedMemory_02_A": "Use a tile size of 16x16, meaning your thread block will also be 16x16.",
"Intermediate_SharedMemory_02_B": "Declare the shared memory tiles using `__shared__ float a_tile[TILE_SIZE][TILE_SIZE];` syntax.",
"Intermediate_SharedMemory_02_C": "Write the loading phase: each thread in the block loads one element of A's tile and one element of B's tile into shared memory.",
"Intermediate_SharedMemory_02_D": "Write the computation phase: after synchronizing, each thread computes its dot product using data exclusively from the shared memory tiles.",
"Intermediate_SharedMemory_02_E": "The kernel will have an outer loop that iterates through the tiles of A and B needed to compute one tile of C.",
"Intermediate_SharedMemory_02_F": "Explain in comments how using shared memory reduces the number of global memory reads.",
"Intermediate_SharedMemory_02_G": "Try using a different tile size, like 32x32, and see how it affects performance. Note that this requires more shared memory and registers.",
"Intermediate_SharedMemory_03": "In your tiled matrix multiplication kernel, place `__syncthreads()` correctly. You need one synchronization after loading the tile of A and another after loading the tile of B, before the computation loop begins for that tile. Add another `__syncthreads()` inside the main loop that iterates over tiles.",
"Intermediate_SharedMemory_03_A": "A common optimization is to load both A and B tiles concurrently, then have a single `__syncthreads()` before the computation loop. Implement this.",
"Intermediate_SharedMemory_03_B": "Explain what would happen if you forgot the `__syncthreads()` after the loading phase. (A thread might try to read a value from shared memory before another thread has written it).",
"Intermediate_SharedMemory_03_C": "Explain what would happen if you forgot the `__syncthreads()` at the end of the main tile loop. (A race condition where threads start loading the next tile before others have finished computing with the current one).",
"Intermediate_SharedMemory_03_D": "Place a `__syncthreads()` inside a conditional branch (`if(threadIdx.x == 0) __syncthreads();`) and explain why this causes a deadlock.",
"Intermediate_SharedMemory_03_E": "Can threads in different blocks synchronize with `__syncthreads()`? Explain why not.",
"Intermediate_SharedMemory_03_F": "Instrument your code with `printf` before and after sync points (from thread 0 only) to trace the execution flow.",
"Intermediate_SharedMemory_04": "Profile and compare the naive vs. tiled matrix multiplication kernels using CUDA events. Measure the execution time for a large matrix (e.g., 1024x1024) and report the speedup achieved by using shared memory.",
"Intermediate_SharedMemory_04_A": "Calculate and report the performance in GFLOPS (billions of floating-point operations per second). The number of FLOPS for matrix multiplication is `2 * N^3`.",
"Intermediate_SharedMemory_04_B": "Use `nsight compute` or `nvprof` to compare the global memory traffic between the two versions. The tiled version should have significantly less traffic.",
"Intermediate_SharedMemory_04_C": "Plot the GFLOPS of both kernels for a range of matrix sizes (e.g., 256, 512, 1024, 2048).",
"Intermediate_SharedMemory_04_D": "Compare your tiled kernel's performance against the `cublasSgemm` function from the cuBLAS library. (cuBLAS will be much faster).",
"Intermediate_SharedMemory_04_E": "Analyze the instruction mix reported by the profiler. The tiled version should have a higher ratio of arithmetic instructions to memory instructions.",
"Intermediate_SharedMemory_04_F": "Profile for different tile sizes (e.g., 8x8, 16x16, 32x32) and find the optimal one for your GPU.",
"Intermediate_SharedMemory_05": "Modify the tiled matrix multiplication to handle matrix dimensions that are not perfect multiples of the tile size. Threads that would read past the matrix boundary should instead load a 0 into the shared memory tile.",
"Intermediate_SharedMemory_05_A": "Add boundary checks to the tile loading phase. `if (row < N && col < N) { a_tile[...] = A[...]; } else { a_tile[...] = 0.0f; }`.",
"Intermediate_SharedMemory_05_B": "The main computation loop within the kernel does not need boundary checks, as it only ever accesses the shared memory tile.",
"Intermediate_SharedMemory_05_C": "Test your modified kernel with non-multiple sizes like 500x500 and verify correctness.",
"Intermediate_SharedMemory_05_D": "Implement this for rectangular matrices as well (e.g. C(M,N) = A(M,K) * B(K,N)).",
"Intermediate_SharedMemory_05_E": "Compare the performance of the boundary-checked kernel on a perfect-multiple size (e.g. 512x512) vs. the original kernel without checks. Is there overhead?",
"Intermediate_SharedMemory_05_F": "An alternative to loading 0 is to make the computation loop conditional. Is this more or less efficient?",
"Intermediate_SharedMemory_06": "Implement a 1D convolution (or a 1D stencil) using shared memory. Each thread block should load a segment of the input array into shared memory, including 'ghost' cells at the boundaries needed by the threads at the edges of the block. Use `__syncthreads()` before computation.",
"Intermediate_SharedMemory_06_A": "For a 3-point stencil `output[i] = w_l*in[i-1] + w_c*in[i] + w_r*in[i+1]`, each thread needs its neighbors. If a block loads N elements for N threads, it must actually load N+2 elements into shared memory.",
"Intermediate_SharedMemory_06_B": "Handle the boundaries of the entire array. Threads in the first and last blocks need special logic to handle elements that don't exist (e.g., assume 0).",
"Intermediate_SharedMemory_06_C": "Implement the loading phase carefully. Each thread can load one element, but some threads (at the edges of the block) might need to load the extra ghost cells.",
"Intermediate_SharedMemory_06_D": "After loading and synchronizing, each thread `i` computes its result using `shared_mem[local_i-1]`, `shared_mem[local_i]`, and `shared_mem[local_i+1]`.",
"Intermediate_SharedMemory_06_E": "Extend this to a 5-point stencil.",
"Intermediate_SharedMemory_06_F": "Compare the performance of the shared memory version to a naive version where each thread reads all its required inputs directly from global memory.",
"Intermediate_SharedMemory_07": "Write a kernel to reverse an array within a single block. Each thread `i` loads `input[i]` into `shared_mem[i]`. After a `__syncthreads()`, thread `i` reads from `shared_mem[BLOCK_SIZE - 1 - i]` and writes to `output[i]`. This is much faster than global memory reversal.",
"Intermediate_SharedMemory_07_A": "Generalize this to reverse a large array by having each block reverse its own segment.",
"Intermediate_SharedMemory_07_B": "Implement an in-place reversal within a block. Thread `i` (where `i < BLOCK_SIZE/2`) swaps `shared_mem[i]` with `shared_mem[BLOCK_SIZE - 1 - i]`.",
"Intermediate_SharedMemory_07_C": "Compare the performance of the shared memory reversal to a naive kernel where `output[i] = input[N-1-i]` for a large array N.",
"Intermediate_SharedMemory_07_D": "Can you reverse an array using only atomic swaps in global memory? Compare the performance.",
"Intermediate_SharedMemory_07_E": "Write a kernel to perform a bit-reversal of an array's elements, a common operation in FFTs.",
"Intermediate_SharedMemory_07_F": "Write a kernel to shift (rotate) an array by `k` positions using shared memory.",
"Intermediate_SharedMemory_08": "Implement a parallel reduction (e.g., sum) within a single block using shared memory. Each thread loads an element into shared memory. Then, in a loop, threads add elements together, halving the number of active threads in each step with `__syncthreads()` in between. The final sum is in `shared_mem[0]`.",
"Intermediate_SharedMemory_08_A": "The reduction loop looks like: `for (unsigned int s = blockDim.x / 2; s > 0; s >>= 1) { if (threadIdx.x < s) { s_data[threadIdx.x] += s_data[threadIdx.x + s]; } __syncthreads(); }` Implement this.",
"Intermediate_SharedMemory_08_B": "Implement a max-finding reduction using the same pattern.",
"Intermediate_SharedMemory_08_C": "Implement a min-finding reduction.",
"Intermediate_SharedMemory_08_D": "This reduction pattern has bank conflicts in the first few iterations. A conflict-free version has the stride `s` decrease from `blockDim.x / 2` but the access is `s_data[i] += s_data[i+s]`. Compare the performance.",
"Intermediate_SharedMemory_08_E": "At the end of the kernel, only thread 0 should write the final result from `shared_mem[0]` to the global memory output.",
"Intermediate_SharedMemory_08_F": "A different pattern is an unrolled loop: `if (blockDim.x >= 512 && threadIdx.x < 256) s_data[threadIdx.x] += s_data[threadIdx.x+256]; __syncthreads();` and so on for 128, 64, etc. This can be faster.",
"Intermediate_SharedMemory_08_G": "For the final 32 elements (one warp), `__syncthreads` is not needed between iterations. Use warp shuffle intrinsics (`__shfl_down_sync`) for a highly optimized final stage of the reduction.",
"Intermediate_SharedMemory_09": "Analyze shared memory bank conflicts. Write a kernel where threads access shared memory with a stride of 1 (`shared_mem[threadIdx.x]`) and another where they access with a stride equal to the number of banks (e.g., 32, `shared_mem[32 * threadIdx.x]`). Profile both and explain the performance difference.",
"Intermediate_SharedMemory_09_A": "Explain what a shared memory bank is and how many there typically are (32).",
"Intermediate_SharedMemory_09_B": "A 2-way bank conflict occurs if two threads in a warp access different locations in the same bank. A 32-way conflict is the worst case. Explain this.",
"Intermediate_SharedMemory_09_C": "Write a kernel where `output[i] = shared_mem[threadIdx.x]`. This should be conflict-free.",
"Intermediate_SharedMemory_09_D": "Write a kernel where `output[i] = shared_mem[threadIdx.x * 2]`. Profile this. On modern GPUs, this may be handled by the cache, but on older ones it would cause 2-way conflicts.",
"Intermediate_SharedMemory_09_E": "Write a kernel where `output[i] = shared_mem[threadIdx.x * 32]`. This should cause severe 32-way bank conflicts. Profile it and compare to the stride-1 case.",
"Intermediate_SharedMemory_09_F": "To fix bank conflicts in a 2D array `shared_mem[ROW][COL]`, padding can be added: `__shared__ float tile[TILE_SIZE][TILE_SIZE+1];`. Explain how accessing `tile[threadIdx.y][threadIdx.x]` could cause conflicts, but accessing `tile[threadIdx.x][threadIdx.y]` is fine.",
"Intermediate_SharedMemory_10": "Implement a more efficient shared-memory transpose for a square tile. Thread `(x, y)` reads from `input(x, y)` and writes to `shared_mem(y, x)`. This avoids bank conflicts that a simple `shared_mem[x][y]` access might cause and helps achieve coalesced global memory reads/writes later.",
"Intermediate_SharedMemory_10_A": "The kernel should load a tile from `input` into shared memory `s_tile` such that `s_tile[threadIdx.y][threadIdx.x] = input[...]`. This read is coalesced.",
"Intermediate_SharedMemory_10_B": "After a `__syncthreads()`, the kernel should write from shared memory to the `output` matrix from a transposed position: `output[...] = s_tile[threadIdx.x][threadIdx.y]` (note x and y are swapped). This write should also be coalesced.",
"Intermediate_SharedMemory_10_C": "The naive transpose reads from `input` uncoalesced and writes coalesced. This version reads coalesced and writes coalesced.",
"Intermediate_SharedMemory_10_D": "This shared memory transpose pattern is a fundamental building block. Implement it within a tile and verify its correctness.",
"Intermediate_SharedMemory_10_E": "Explain why reading `s_tile[threadIdx.y][threadIdx.x]` is fine but writing `s_tile[threadIdx.y][threadIdx.x]` could cause bank conflicts if `threadIdx.y` is the faster-changing index.",
"Intermediate_SharedMemory_10_F": "Use padding in the shared memory array (`__shared__ float s_tile[TILE_SIZE][TILE_SIZE+1]`) to eliminate bank conflicts even with non-optimal access patterns.",
"Intermediate_MemoryCoalescing_01": "Write a naive matrix transpose kernel where thread `(x, y)` reads `input[y][x]` and writes to `output[x][y]`. Assuming row-major layout, analyze why the reads from `input` will be uncoalesced (strided) and hurt performance.",
"Intermediate_MemoryCoalescing_01_A": "Explain what memory coalescing is. (Threads in a warp accessing contiguous memory locations in a single transaction).",
"Intermediate_MemoryCoalescing_01_B": "For the read `input[y * width + x]`, threads in a warp have the same `y` and consecutive `x`. This is coalesced.",
"Intermediate_MemoryCoalescing_01_C": "For the read `input[y][x]` in a transpose, thread `(x,y)` reads `input[y*width + x]`.  If threads in a warp have consecutive `x` values (coalesced) and the same `y`, then consecutive threads read from `input[y*width + 0]`, `input[y*width + 1]`, etc. which is COALESCED. The question is slightly misstated.  Let's rephrase: `thread(x,y)` computes `output[y][x] = input[x][y]`. The read from `input` is now strided. Let's assume this is the intended question.",
"Intermediate_MemoryCoalescing_01_D": "Assuming the corrected problem: `output[y*width+x] = input[x*height+y]`. The reads from `input` by consecutive threads in a warp (with consecutive `x`) will access memory locations separated by `height`, which is highly uncoalesced.",
"Intermediate_MemoryCoalescing_01_E": "Profile this corrected naive transpose kernel.",
"Intermediate_MemoryCoalescing_01_F": "Implement the kernel using a 2D grid of 2D blocks.",
"Intermediate_MemoryCoalescing_02": "Write an optimized matrix transpose using shared memory to achieve coalescing. Each thread block reads a tile from the input matrix into a shared memory buffer with coalesced reads. Then, after a sync, it writes from shared memory to the output matrix in a coalesced pattern.",
"Intermediate_MemoryCoalescing_02_A": "Phase 1: Coalesced read from `input`. Thread `(tx,ty)` in a block reads from `input` at a location corresponding to its `(global_x, global_y)` and stores it in `tile[ty][tx]`.",
"Intermediate_MemoryCoalescing_02_B": "Phase 2: `__syncthreads()` to ensure the whole tile is loaded.",
"Intermediate_MemoryCoalescing_02_C": "Phase 3: Coalesced write to `output`. Thread `(tx,ty)` reads from `tile[tx][ty]` (the transposed location) and writes to the transposed global position in `output`.",
"Intermediate_MemoryCoalescing_02_D": "This pattern ensures both global reads and global writes are fully coalesced.",
"Intermediate_MemoryCoalescing_02_E": "Implement this using a 16x16 or 32x32 tile size.",
"Intermediate_MemoryCoalescing_02_F": "Handle non-square matrices and dimensions that are not multiples of the tile size.",
"Intermediate_MemoryCoalescing_03": "Profile the naive and shared-memory-based transpose kernels. Use `nvprof` or NVIDIA Nsight Compute to measure the global memory transaction count and observed bandwidth for both. The difference should be significant.",
"Intermediate_MemoryCoalescing_03_A": "Report the execution time speedup of the optimized version over the naive one.",
"Intermediate_MemoryCoalescing_03_B": "Look for the `gld_transactions` and `gst_transactions` metrics in the profiler. The naive version should have many more read transactions (`gld`).",
"Intermediate_MemoryCoalescing_03_C": "Observe the 'Achieved Occupancy' metric. How does it compare between the two kernels?",
"Intermediate_MemoryCoalescing_03_D": "Plot the effective bandwidth (GB/s) of both kernels for various matrix sizes.",
"Intermediate_MemoryCoalescing_03_E": "Write a summary of why the shared memory version is faster, referencing the profiler metrics.",
"Intermediate_MemoryCoalescing_04": "Write a kernel that copies an array, but with a strided access pattern: `output[i] = input[i * STRIDE]`. Run this for `STRIDE=1` and `STRIDE=2`. Profile both and observe the drop in memory bandwidth for the strided case due to uncoalesced access.",
"Intermediate_MemoryCoalescing_04_A": "Run for `STRIDE=32`. The performance should be very poor.",
"Intermediate_MemoryCoalescing_04_B": "Explain what happens when a warp executes a strided read. How many memory transactions are issued for `STRIDE=2`? For `STRIDE=32`?",
"Intermediate_MemoryCoalescing_04_C": "Now test a strided write: `output[i * STRIDE] = input[i]`. Profile and compare.",
"Intermediate_MemoryCoalescing_04_D": "Could shared memory be used to fix this problem? Describe a strategy where a block would perform a strided read into shared memory, sync, and then a coalesced write from shared memory to the output (or vice versa).",
"Intermediate_MemoryCoalescing_04_E": "Use Nsight Compute to visualize the memory chart and see the inefficiency of the strided access.",
"Intermediate_MemoryCoalescing_05": "Write a kernel that accesses a struct array: `struct { float x, y, z; }`. Compare the performance of an Array of Structs (AoS) access pattern (`my_structs[i].x`) versus a Struct of Arrays (SoA) pattern (`my_xs[i]`, `my_ys[i]`, `my_zs[i]`). SoA is usually much better for coalescing.",
"Intermediate_MemoryCoalescing_05_A": "Implement the AoS version: create `MyStruct* d_structs`. Write a kernel that reads only the `x` component from each struct: `output[i] = d_structs[i].x`.",
"Intermediate_MemoryCoalescing_05_B": "Analyze why the AoS kernel has poor memory performance. (Threads in a warp access `x`, but the `y` and `z` components are loaded into cache unnecessarily, wasting bandwidth).",
"Intermediate_MemoryCoalescing_05_C": "Implement the SoA version: create `float* d_xs, *d_ys, *d_zs`. Write a kernel that reads from `d_xs`: `output[i] = d_xs[i]`.",
"Intermediate_MemoryCoalescing_05_D": "Analyze why the SoA kernel has excellent memory performance (fully coalesced).",
"Intermediate_MemoryCoalescing_05_E": "Profile and time both versions and report the speedup of SoA over AoS.",
"Intermediate_MemoryCoalescing_05_F": "What if the kernel needed to access `x`, `y`, and `z`? `output[i] = d_structs[i].x + d_structs[i].y + d_structs[i].z;`. Compare this to the SoA version. The performance gap might narrow.",
"Intermediate_Synchronization_01": "Implement a parallel sum reduction for a large array that does not fit in a single block. The first kernel launch reduces each block's segment of the array into a partial sum (using the shared memory technique). A second, smaller kernel launch then sums up these partial results.",
"Intermediate_Synchronization_01_A": "The first kernel will take the large input array and an output array sized to the number of blocks. Each block writes its partial sum to `d_partial_sums[blockIdx.x]`.",
"Intermediate_Synchronization_01_B": "The second kernel is launched with just a single block. This block loads the `d_partial_sums` array into shared memory and performs a final reduction on it.",
"Intermediate_Synchronization_01_C": "Generalize this to require more than two passes if the number of partial sums is still too large for a single block.",
"Intermediate_Synchronization_01_D": "An alternative to a second kernel is to copy the `d_partial_sums` array back to the host and sum it on the CPU.",
"Intermediate_Synchronization_01_E": "Combine the approaches: a single kernel computes block-level partial sums, and then thread 0 of each block uses `atomicAdd` on a single global counter (`*d_final_sum`) to add its result.",
"Intermediate_Synchronization_01_F": "Implement a version where the first kernel uses a grid-stride loop, so each thread processes multiple input elements before the block-level reduction begins.",
"Intermediate_Synchronization_02": "Implement a parallel maximum-finding algorithm using the same two-pass reduction strategy as the sum reduction.",
"Intermediate_Synchronization_02_A": "The reduction operation in shared memory is now `s_data[i] = max(s_data[i], s_data[i+s])` instead of `+=`.",
"Intermediate_Synchronization_02_B": "Implement the two-kernel approach for finding the maximum value.",
"Intermediate_Synchronization_02_C": "Modify the algorithm to find not just the maximum value, but also the index of that value. This requires carrying a struct `{value, index}` through the reduction.",
"Intermediate_Synchronization_02_D": "Implement a parallel minimum-finding algorithm.",
"Intermediate_Synchronization_02_E": "Compare the performance of your two-pass reduction with Thrust's `thrust::reduce` or `thrust::max_element`.",
"Intermediate_Synchronization_02_F": "Use `atomicMax` to implement a single-kernel max reduction and compare its performance to the two-pass method.",
"Intermediate_Synchronization_03": "Explain what a 'race condition' is in the context of CUDA. Write a kernel where multiple threads attempt to increment the same global memory location without atomics (`output[0] = output[0] + 1`). Run it and show that the final result is incorrect.",
"Intermediate_Synchronization_03_A": "The incorrect increment is a 'read-modify-write' race. Explain the sequence: all threads read the old value, all threads compute the new value, all threads write their new value. The last one to write wins.",
"Intermediate_Synchronization_03_B": "Launch the kernel with 256 threads. The expected result is 256. What result do you actually get? (Something much smaller).",
"Intermediate_Synchronization_03_C": "Run the program multiple times and show that the incorrect result is also non-deterministic.",
"Intermediate_Synchronization_03_D": "Create a race condition on an array: have all threads increment `output[threadIdx.x % 4]`. Some locations will have races, others won't.",
"Intermediate_Synchronization_03_E": "Run your racy kernel with `cuda-memcheck --tool racecheck` and analyze the output.",
"Intermediate_Synchronization_03_F": "Is there a race condition if multiple threads only *read* from the same location? (No, this is safe and common).",
"Intermediate_Synchronization_04": "Fix the race condition from the previous problem by using an atomic operation: `atomicAdd(&output[0], 1)`. Verify that the result is now correct.",
"Intermediate_Synchronization_04_A": "Explain what an atomic operation is (an indivisible operation that completes without interruption).",
"Intermediate_Synchronization_04_B": "Time the naive (racy) kernel vs. the `atomicAdd` kernel. Atomic operations have a performance cost. Is it significant here?",
"Intermediate_Synchronization_04_C": "Implement increment using `atomicCAS` (Compare-And-Swap) in a loop: `int old = *addr; while (old != atomicCAS(addr, old, old+1)) { old = *addr; }`.",
"Intermediate_Synchronization_04_D": "Use other atomic functions like `atomicSub`, `atomicMin`, `atomicMax`, `atomicExch`.",
"Intermediate_Synchronization_04_E": "Are atomics supported on shared memory? (Yes). Write a reduction that uses atomics on a shared memory location instead of the synchronized strided reduction.",
"Intermediate_Synchronization_04_F": "Atomics on floating point numbers were not always supported. Check your device's compute capability. Modern GPUs support them for `float` and `double`.",
"Intermediate_Synchronization_05": "Implement a parallel histogram calculation for an array of integers. Multiple threads will process the input array and may try to increment the same histogram bin simultaneously. Use `atomicAdd` on the bin counters to prevent data races and ensure a correct histogram.",
"Intermediate_Synchronization_05_A": "The kernel will take an `int* input_data` and an `int* bins`. Each thread processes one or more elements. For each element `val`, it does `atomicAdd(&bins[val], 1)`.",
"Intermediate_Synchronization_05_B": "Handle input values that are outside the range of the histogram bins.",
"Intermediate_Synchronization_05_C": "Initialize the bin array to all zeros using `cudaMemset` before launching the kernel.",
"Intermediate_Synchronization_05_D": "A performance optimization: have each thread compute a private histogram in registers or shared memory first. Then, at the end of the thread, atomically add the private histogram's counts to the global bins. This reduces atomic contention.",
"Intermediate_Synchronization_05_E": "Verify the GPU histogram result against a simple CPU implementation.",
"Intermediate_Synchronization_05_F": "Profile the histogram kernel. The performance will be limited by the contention on the atomic operations.",
"Intermediate_Synchronization_06": "Write a kernel that uses `__syncthreads()` incorrectly, for example, inside a conditional branch (`if (threadIdx.x < 16) __syncthreads();`). Compile and run this. Explain why this leads to a deadlock and will cause the kernel to hang or error out.",
"Intermediate_Synchronization_06_A": "Explain thread divergence. When a conditional is met by some threads in a warp but not others, the warp executes both paths.",
"Intermediate_Synchronization_06_B": "In the deadlock example, threads < 16 reach the barrier, but threads >= 16 do not. The first group waits forever for the second group, which will never arrive.",
"Intermediate_Synchronization_06_C": "Run this kernel with `cuda-memcheck --tool synccheck`. Analyze its output.",
"Intermediate_Synchronization_06_D": "Show a valid use of `__syncthreads()` inside a conditional, where the condition is uniform across the entire block (e.g., `if (blockIdx.x == 0) { ... __syncthreads(); }`).",
"Intermediate_Synchronization_06_E": "Try using `__syncthreads()` inside a loop where some threads might exit the loop earlier than others. This will also cause a deadlock.",
"Intermediate_Synchronization_06_F": "The CUDA compiler is often smart enough to detect simple cases of this error. Does your example compile without warnings?",
"Intermediate_Synchronization_07": "Implement a simple spin-lock in global memory using `atomicCAS` (Compare-And-Swap). A thread tries to acquire the lock by swapping a 0 to a 1. If it succeeds, it enters a critical section. It releases the lock by writing 0 back. (Note: This is often inefficient on GPUs but is a good exercise).",
"Intermediate_Synchronization_07_A": "The lock acquisition looks like: `while(atomicCAS(lock_ptr, 0, 1) != 0);`.",
"Intermediate_Synchronization_07_B": "The lock release can be a simple `*lock_ptr = 0;`, but it should be a volatile write to ensure visibility: `*((volatile int*)lock_ptr) = 0;` or `atomicExch(lock_ptr, 0)`.",
"Intermediate_Synchronization_07_C": "Use this lock to protect the non-atomic increment from the earlier race condition exercise. Only one thread at a time will be able to perform the read-modify-write.",
"Intermediate_Synchronization_07_D": "Profile the spin-lock version against the `atomicAdd` version. The spin-lock will be much, much slower due to serialization.",
"Intermediate_Synchronization_07_E": "Explain why spin-locks are generally a bad idea on GPUs. (Massive thread counts cause extreme contention; threads that are spinning are wasting execution resources and preventing other warps from running).",
"Intermediate_Synchronization_07_F": "What is a better approach than a global lock for managing contention? (Lock-free algorithms, privatization, finer-grained locks).",
"Intermediate_Streams_01": "Write a program that processes a large dataset by breaking it into N chunks. Perform the following sequence N times in a loop on the default stream: (1) Copy HtoD, (2) Execute Kernel, (3) Copy DtoH. Time the total operation.",
"Intermediate_Streams_01_A": "Use pinned host memory (`cudaMallocHost`) for your data buffers.",
"Intermediate_Streams_01_B": "Implement the logic. You'll need one device buffer big enough for one chunk. The host loop will copy a chunk, call the kernel, and copy the result back for each chunk.",
"Intermediate_Streams_01_C": "A `cudaDeviceSynchronize()` is implicitly happening after each operation, making it fully synchronous. Time the total execution of the loop.",
"Intermediate_Streams_01_D": "The kernel could be a simple vector squaring or SAXPY operation.",
"Intermediate_Streams_01_E": "Use Nsight Systems (`nsys`) to visualize the execution timeline. You should see a serial pattern: [COPY]...[KERNEL]...[COPY]...",
"Intermediate_Streams_01_F": "Use a single large allocation on the host and use pointer offsets to process each chunk.",
"Intermediate_Streams_02": "Modify the previous program to use two CUDA streams. Overlap execution by issuing the work for chunk `k` and chunk `k+1` in a pipelined fashion: Copy(k, s1), Copy(k+1, s2), Exec(k, s1), Exec(k+1, s2), etc. Use `cudaStreamSynchronize` or CUDA events to manage dependencies. The host memory must be pinned.",
"Intermediate_Streams_02_A": "Create two streams: `cudaStream_t stream1, stream2; cudaStreamCreate(...)`.",
"Intermediate_Streams_02_B": "You will need two sets of device buffers, one for each stream.",
"Intermediate_Streams_02_C": "The main loop will be more complex. A common pattern is to prime the pipeline, then loop: `for k=0 to N-1: stream = streams[k%2]; ... issue async work on stream... cudaStreamSynchronize(prev_stream); ... process result from prev_stream ...`",
"Intermediate_Streams_02_D": "Use `cudaMemcpyAsync` and pass the stream argument to all copies and kernel launches.",
"Intermediate_Streams_02_E": "Use Nsight Systems (`nsys`) to visualize the timeline. You should now see the HtoD copy for chunk `k+1` overlapping with the kernel execution for chunk `k`.",
"Intermediate_Streams_02_F": "Remember to destroy the streams with `cudaStreamDestroy` at the end.",
"Intermediate_Streams_03": "Measure the performance improvement of the streamed version over the sequential version. The overlap of copy and execution should result in a significant speedup, hiding the data transfer latency.",
"Intermediate_Streams_03_A": "Calculate the speedup factor (Sequential Time / Streamed Time).",
"Intermediate_Streams_03_B": "The theoretical maximum speedup depends on the relative times of Copy, Exec, and Copy. If T_copy_HtoD + T_copy_DtoH > T_exec, the program is memory-bound and overlap is very effective.",
"Intermediate_Streams_03_C": "Try changing the 'work' done by the kernel. If the kernel is very fast, the speedup will be high. If the kernel is very slow (compute-bound), the speedup will be lower because the copies finish long before the kernel does.",
"Intermediate_Streams_03_D": "Extend the pipeline to use 3 streams and 3 buffers. Does this provide further speedup over 2 streams?",
"Intermediate_Streams_03_E": "Make sure you are using pinned host memory. The overlap will not occur with pageable memory.",
"Intermediate_Streams_03_F": "Plot the total execution time vs. the number of streams used (1, 2, 3, 4...). The benefit usually saturates quickly.",
"Intermediate_Streams_04": "Create a dependency between two streams. Launch a kernel on stream1. Record an event on stream1 after the kernel. Then, make stream2 wait for that event using `cudaStreamWaitEvent` before launching its own kernel. This ensures the second kernel only starts after the first one is finished.",
"Intermediate_Streams_04_A": "Create two streams and one `cudaEvent_t`.",
"Intermediate_Streams_04_B": "The sequence is: `kernelA<<<..., stream1>>>();`, `cudaEventRecord(event, stream1);`, `cudaStreamWaitEvent(stream2, event, 0);`, `kernelB<<<..., stream2>>>();`.",
"Intermediate_Streams_04_C": "Explain why this is more efficient than using `cudaDeviceSynchronize()` between the kernels (it doesn't stall the host CPU and only synchronizes the necessary streams).",
"Intermediate_Streams_04_D": "Use Nsight Systems to visualize the dependency. You will see stream2 being idle until the event on stream1 is triggered.",
"Intermediate_Streams_04_E": "Can a stream wait on an event that has not yet been recorded? (Yes, it will wait until it is recorded and completed).",
"Intermediate_Streams_04_F": "What is the difference between `cudaStreamWaitEvent` and `cudaEventSynchronize`? (The former is a non-blocking device-side dependency, the latter is a blocking host-side wait).",
"Intermediate_Streams_05": "Write a program with a 'callback'. Use `cudaLaunchHostFunc` to queue a CPU function to be executed after all preceding work in a stream is complete. The CPU function can, for example, print a message indicating a chunk is done.",
"Intermediate_Streams_05_A": "The callback function must have the signature `void CUDART_CB my_callback(void* user_data)`.",
"Intermediate_Streams_05_B": "The sequence is: `kernel<<<..., stream>>>();`, `cudaLaunchHostFunc(stream, my_callback, (void*)some_data);`.",
"Intermediate_Streams_05_C": "Demonstrate that the callback function is executed asynchronously from the host thread, and only after the kernel finishes.",
"Intermediate_Streams_05_D": "Pass a pointer to a struct or class as the `user_data` to give the callback function context about the work that was completed.",
"Intermediate_Streams_05_E": "Use a callback in your multi-chunk processing loop. When a chunk's DtoH copy is finished, the callback can trigger the host-side processing of that chunk's data, further improving pipelining.",
"Intermediate_Streams_05_F": "What are the restrictions on what the callback function can do? (It should not call most CUDA API functions, especially synchronous ones).",
"Advanced_MultiGPU_01": "Write a program that enumerates all GPUs on the system, and for each one, prints its name and memory capacity. Then, explicitly set the active device to GPU 1 (if it exists) using `cudaSetDevice` and run a simple kernel on it.",
"Advanced_MultiGPU_01_A": "Write a program that allocates 100MB of memory on every available GPU.",
"Advanced_MultiGPU_01_B": "Write a program that finds the GPU with the most available free memory at runtime (using `cudaMemGetInfo`) and selects it to run a computation.",
"Advanced_MultiGPU_01_C": "Create two host threads. Have one thread set device 0 and run a kernel, while the second thread sets device 1 and runs a kernel concurrently.",
"Advanced_MultiGPU_02": "Split a large vector addition task across two GPUs. Allocate half of the A and B vectors on GPU 0 and the other half on GPU 1. Launch a kernel on each GPU to process its half. The host must manage both devices, switching context with `cudaSetDevice`.",
"Advanced_MultiGPU_02_A": "After the kernels complete, copy the results from both GPUs back to the host and assemble the final complete vector C.",
"Advanced_MultiGPU_02_B": "Use CUDA streams to launch the work on both GPUs asynchronously from the host's perspective, then wait for both to complete.",
"Advanced_MultiGPU_02_C": "Generalize the program to split the work across all available GPUs on the system.",
"Advanced_MultiGPU_03": "Enable peer-to-peer (P2P) access between two GPUs using `cudaDeviceCanAccessPeer` and `cudaDeviceEnablePeerAccess`. Write a program where GPU 0 directly reads from a memory buffer located on GPU 1 using `cudaMemcpyPeerAsync`, bypassing host memory.",
"Advanced_MultiGPU_03_A": "Measure the bandwidth of the P2P `cudaMemcpy` and compare it to the bandwidth of a manual copy through the host (GPU1 -> Host -> GPU0).",
"Advanced_MultiGPU_03_B": "Write a kernel running on GPU 0 that takes a pointer to memory allocated on GPU 1 and accesses it directly (this requires Unified Virtual Addressing).",
"Advanced_MultiGPU_03_C": "Check for P2P support between all pairs of GPUs on your system and print a compatibility matrix.",
"Advanced_MultiGPU_04": "Implement a 'halo exchange' pattern between two GPUs. Each GPU works on a sub-domain of a 1D array. After a computation step, each GPU needs the boundary data from its neighbor. Use P2P `cudaMemcpy` to exchange these 'halo' regions directly.",
"Advanced_MultiGPU_04_A": "The exchange involves GPU 0 sending its rightmost element to GPU 1's 'left halo' region, and GPU 1 sending its leftmost element to GPU 0's 'right halo' region.",
"Advanced_MultiGPU_04_B": "Use streams and events to overlap the halo exchange with computation on the interior of each sub-domain.",
"Advanced_MultiGPU_04_C": "Extend the halo exchange pattern to a ring of all available GPUs, where each GPU communicates with two neighbors.",
"Advanced_Libraries_01": "Perform a large matrix-matrix multiplication (SGEMM for floats) using the cuBLAS library. Initialize matrices A and B on the host, transfer them to the device, create a cuBLAS handle, and call the `cublasSgemm` function. Compare its performance to your custom kernel.",
"Advanced_Libraries_01_A": "Use cuBLAS to perform a matrix-vector multiplication (`cublasSgemv`).",
"Advanced_Libraries_01_B": "Use cuBLAS to perform a vector dot product (`cublasSdot`).",
"Advanced_Libraries_01_C": "Read the cuBLAS documentation for `cublasSgemm` and understand the parameters for transposing matrices and scaling factors (alpha and beta). Use it to compute `C = 2.0*A*B + 0.5*C`.",
"Advanced_Libraries_02": "Use the cuRAND library to generate a large array of random numbers directly on the GPU. Create a generator, set its seed, and call `curandGenerateUniform` to fill a device array with floating-point numbers between 0.0 and 1.0.",
"Advanced_Libraries_02_A": "Use `curandGenerateNormal` to generate normally distributed (Gaussian) random numbers.",
"Advanced_Libraries_02_B": "Generate random integers instead of floats.",
"Advanced_Libraries_02_C": "Create multiple generators on different streams to generate random numbers in parallel for different purposes.",
"Advanced_Libraries_03": "Use the Thrust library to find the sum of a large array of integers on the GPU. Include the Thrust headers, create `thrust::device_vector` wrappers for your data, and use `thrust::reduce` to perform the parallel reduction with a single line of C++ code.",
"Advanced_Libraries_03_A": "Use `thrust::transform_reduce` to compute a dot product of two vectors in a single call.",
"Advanced_Libraries_03_B": "Use `thrust::min_element` and `thrust::max_element` to find the minimum and maximum values in a `device_vector`.",
"Advanced_Libraries_03_C": "Use a `thrust::counting_iterator` to initialize a `device_vector` with the sequence 0, 1, 2, ... without using any host data.",
"Advanced_Libraries_04": "Use Thrust to sort a large array of key-value pairs. Create one `device_vector` for keys and another for values. Use `thrust::sort_by_key` to sort both vectors based on the key values.",
"Advanced_Libraries_04_A": "Use `thrust::sort` to sort a single `device_vector`.",
"Advanced_Libraries_04_B": "Use `thrust::unique` to count the number of unique elements in a sorted vector.",
"Advanced_Libraries_04_C": "Define a custom comparison operator and pass it to `thrust::sort` to sort in descending order.",
"Advanced_Libraries_05": "Use the CUB library's `DeviceScan::InclusiveSum` to perform a prefix sum (scan) operation on a large device array. This is a fundamental parallel building block that is highly optimized in CUB.",
"Advanced_Libraries_05_A": "Use CUB's `DeviceReduce::Sum` to perform a sum reduction and compare its performance to your own and Thrust's.",
"Advanced_Libraries_05_B": "Use CUB's `DeviceRadixSort::SortPairs` to sort key-value pairs.",
"Advanced_Libraries_05_C": "Integrate a CUB block-level primitive, like `BlockScan`, into your own custom kernel to perform a scan within a single thread block.",
"Advanced_ModernFeatures_01": "Re-implement vector addition using Unified Memory. Allocate all vectors using `cudaMallocManaged`. Notice that you no longer need explicit `cudaMemcpy` calls. The CUDA runtime will migrate data on-demand.",
"Advanced_ModernFeatures_01_A": "Use `cudaMemAdvise` to give the driver hints about data usage patterns (e.g., set a vector to `cudaMemAdviseSetReadMostly` on the device).",
"Advanced_ModernFeatures_01_B": "Use `cudaMemPrefetchAsync` to proactively migrate data to a specific processor (CPU or a specific GPU) before it's accessed to hide migration latency.",
"Advanced_ModernFeatures_01_C": "Profile the on-demand migration version vs. the prefetching version. Use Nsight Systems to visualize the page faults and data migrations.",
"Advanced_ModernFeatures_02": "Write a program that uses Cooperative Groups to perform a grid-wide synchronization. Launch a kernel and get the `grid_group`. Use the group's `sync()` method to ensure all threads in the entire grid have reached a certain point before proceeding. This allows for single-kernel reductions on large arrays.",
"Advanced_ModernFeatures_02_A": "Implement a single-kernel sum reduction using a grid group sync. A first phase has all threads reduce data into shared memory. After a block sync, thread 0 of each block atomically adds its partial sum to a global total. After a `grid.sync()`, thread 0 of block 0 reads the final total.",
"Advanced_ModernFeatures_02_B": "Use `cooperative_groups::thread_block_tile<32>` to perform a warp-level reduction using warp-synchronous intrinsics, which is more efficient than using shared memory for the last 32 values.",
"Advanced_ModernFeatures_02_C": "To use cooperative groups for grid-wide sync, the kernel must be launched with `cudaLaunchCooperativeKernel`. Adapt your launch code.",
"Advanced_ModernFeatures_03": "Implement a simple program using Dynamic Parallelism. A parent kernel is launched from the host. Inside this parent kernel, based on some condition, it launches a child kernel. For example, the parent kernel iterates over a data structure, launching a child kernel to process each node.",
"Advanced_ModernFeatures_03_A": "Write a parent kernel that launches a simple child kernel (e.g., a `printf`) with a configuration of 1 block and 1 thread.",
"Advanced_ModernFeatures_03_B": "Create a parent kernel that launches a dynamically sized grid for a child kernel based on data computed within the parent.",
"Advanced_ModernFeatures_03_C": "To compile code with dynamic parallelism, you need to add `-rdc=true` (relocatable device code) to your `nvcc` flags. Practice this.",
"Advanced_ModernFeatures_04": "For a GPU with Tensor Cores (Volta or newer), use the `nvcuda::wmma` (Warp-Level Matrix-Multiply-Accumulate) C++ API to perform a small 16x16 matrix multiplication. This involves loading fragments into registers, performing the `mma_sync` operation, and storing the results. Compare its speed to a standard CUDA core implementation.",
"Advanced_ModernFeatures_04_A": "Extend the WMMA kernel to perform a larger matrix multiplication by tiling the operation, where each warp is responsible for computing a 16x16 output tile.",
"Advanced_ModernFeatures_04_B": "Use WMMA to multiply matrices of `half` precision numbers, which is the primary use case for Tensor Cores.",
"Advanced_ModernFeatures_04_C": "Profile your WMMA kernel with Nsight Compute and look for the `tensor_precision_fu_utilization` metric to confirm Tensor Cores are being used.",
"Advanced_ModernFeatures_05": "Implement a simple program that uses CUDA Inter-Process Communication (IPC). Process A allocates a chunk of GPU memory and gets an IPC handle for it (`cudaIpcGetMemHandle`). Process A passes this handle to Process B (e.g., via a file). Process B uses the handle to open the memory (`cudaIpcOpenMemHandle`) and can then read/write to Process A's GPU memory directly.",
"Advanced_ModernFeatures_05_A": "Extend the IPC example to share a CUDA event. Process A records an event, gets an IPC handle for it, passes it to Process B, which then waits on that event.",
"Advanced_ModernFeatures_05_B": "Demonstrate that after Process A exits, the memory it shared via IPC is no longer valid for Process B to access.",
"Advanced_ModernFeatures_05_C": "Use IPC to implement a simple client-server model where a server process manages a large dataset on the GPU, and multiple client processes can attach to its memory to perform read-only computations."
}
